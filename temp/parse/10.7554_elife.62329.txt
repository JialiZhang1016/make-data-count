 
1 
EXPOSING DISTINCT SUBCORTICAL COMPONENTS OF THE AUDITORY BRAINSTEM RESPONSE 
1 
EVOKED BY CONTINUOUS NATURALISTIC SPEECH 
2 
Melissa J Polonenko1, 3, 4, Ph.D. (ORCID: 0000-0003-1914-6117) 
3 
Ross K Maddox, Ph.D.1, 2, 3, 4, * (ORCID: 0000-0003-2668-0238) 
4 
 
5 
1 Department of Neuroscience, University of Rochester 
6 
2 Department of Biomedical Engineering, University of Rochester 
7 
3 Del Monte Institute for Neuroscience, University of Rochester 
8 
4 Center for Visual Science, University of Rochester 
9 
 
10 
* to whom correspondence should be addressed: 
11 
Ross Maddox 
12 
University of Rochester 
13 
Goergen Hall 
14 
Box 270168 
15 
Rochester, NY 14627 
16 
Email: ross.maddox@rochester.edu  
17 
 
 
18 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
2 
ABSTRACT  
19 
Speech processing is built upon encoding by the auditory nerve and brainstem, yet we know very 
20 
little about how these processes unfold in specific subcortical structures. These structures are deep and 
21 
respond quickly, making them difficult to study during ongoing speech. Recent techniques begin to address 
22 
this problem, but yield temporally broad responses with consequently ambiguous neural origins. Here we 
23 
describe a method that pairs re-synthesized “peaky” speech with deconvolution analysis of EEG 
24 
recordings. We show that in adults with normal hearing, the method quickly yields robust responses whose 
25 
component waves reflect activity from distinct subcortical structures spanning auditory nerve to rostral 
26 
brainstem. We further demonstrate the versatility of peaky speech by simultaneously measuring bilateral 
27 
and ear-specific responses across different frequency bands, and discuss important practical 
28 
considerations such as talker choice. The peaky speech method holds promise as a tool for investigating 
29 
speech encoding and processing, and for clinical applications.  
30 
 
31 
KEYWORDS 
32 
speech, auditory brainstem response, evoked potentials, electroencephalography, assessment 
33 
 
 
34 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
3 
INTRODUCTION 
35 
Understanding speech is an important, complex process that spans the auditory system from 
36 
cochlea to cortex. A temporally precise network transforms the strikingly dynamic fluctuations in amplitude 
37 
and spectral content of natural, ongoing speech into meaningful information, and modifies that information 
38 
based on attention or other priors (Mesgarani et al., 2009). Subcortical structures play a critical role in this 
39 
process – they do not merely relay information from the periphery to the cortex, but also perform important 
40 
functions for speech understanding, such as localizing sound (e.g., Grothe and Pecka, 2014) and encoding 
41 
vowels across different levels and in background noise (e.g., Carney et al., 2015). Furthermore, subcortical 
42 
structures receive descending information from the cortex through corticofugal pathways (Bajo et al., 2010; 
43 
Bajo and King, 2012; Winer, 2005), suggesting they may also play an important role in modulating speech 
44 
and auditory streaming. Given the complexity of speech processing, it is important to parse and understand 
45 
contributions from different neural generators. However, these subcortical structures are deep and respond 
46 
to stimuli with very short latencies, making them difficult to study during ecologically-salient stimuli such as 
47 
continuous and naturalistic speech. We created a novel paradigm aimed at elucidating the contributions 
48 
from distinct subcortical structures to ongoing, naturalistic speech.  
49 
Activity in deep brainstem structures can be “imaged” by the latency of waves in a surface electrical 
50 
potential (electroencephalography, EEG) called the auditory brainstem response (ABR). The ABR’s 
51 
component waves have been attributed to activity in different subcortical structures with characteristic 
52 
latencies: the auditory nerve contributes to waves I and II (~1.5–3 ms), the cochlear nucleus to wave III (~4 
53 
ms), the superior olivary complex and lateral lemniscus to wave IV (~ 5 ms), and the lateral lemniscus and 
54 
inferior colliculus to wave V (~6 ms) (Møller and Jannetta, 1983; review by Moore, 1987; Starr and 
55 
Hamilton, 1976). Waves I, III, and V are most often easily distinguished in the human response. Subcortical 
56 
structures may also contribute to the earlier P0 (12–14 ms) and Na (15–25 ms) waves (Hashimoto, 1982; 
57 
Kileny et al., 1987; Picton et al., 1974) of the middle latency response (MLR), which are then followed by 
58 
thalamo-cortically generated waves Pa, Nb, and Pb/P1 (Geisler et al., 1958; Goldstein and Rodman, 1967). 
59 
ABR and MLR waves have a low signal-to-noise ratio (SNR) and require numerous stimulus repetitions to 
60 
record a good response. Furthermore, they are quick and often occur before the stimulus has ended. 
61 
Therefore, out of necessity, most human brainstem studies have focused on brief stimuli such as clicks, 
62 
tone pips, or speech syllables, rather than more natural speech.  
63 
Recent analytical techniques have overcome limitations on stimuli, allowing continuous naturally 
64 
uttered speech to be used. One such technique extracts the fundamental waveform from the speech 
65 
stimulus and finds the envelope of the cross-correlation between that waveform and the recorded EEG 
66 
data (Forte et al., 2017). The response has an average peak time of about 9 ms, with contributions 
67 
primarily from the inferior colliculus (Saiz-Alia and Reichenbach, 2020). A second technique considers the 
68 
rectified broadband speech waveform as the input to a linear system and the EEG data as the output, and 
69 
uses deconvolution to compute the ABR waveform as the impulse response of the system (Maddox and 
70 
Lee, 2018). The speech-derived ABR shows a wave V peak whose latency is highly correlated with the 
71 
click response wave V across subjects, demonstrating that the component is generated in the rostral 
72 
brainstem. A third technique averages responses to each chirp (click-like transients that quickly increase in 
73 
frequency) in re-synthesized “cheech" stimuli (CHirp spEECH; Miller et al., 2017) that interleaves 
74 
alternating octave frequency bands of speech and of chirps aligned with some glottal pulses (Backer et al., 
75 
2019). Brainstem responses to these stimuli also show a wave V, but do not show earlier waves unless 
76 
presented monaurally over headphones (Backer et al., 2019; Miller et al., 2017). While these methods 
77 
reflect subcortical activity, the first two provide temporally broad responses with a lack of specificity 
78 
regarding underlying neural sources. None of the three methods shows the earlier canonical components 
79 
such as waves I and III that would allow rostral brainstem activity to be distinguished from, for example, the 
80 
auditory nerve. Such activity is important to assess, especially given the current interest in the potential 
81 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
4 
contributions of auditory nerve loss in disordered processing of speech in noise (Bramhall et al., 2019; 
82 
Liberman et al., 2016; Prendergast et al., 2017).  
83 
Although click responses can assess sound encoding (of clicks) at early stages of the auditory 
84 
system, a speech-evoked response with the same components would assess region-specific encoding 
85 
within the acoustical context of the dynamic spectrotemporal characteristics of speech – information that is 
86 
not possible to obtain from click responses. Furthermore, changes to the amplitudes and latencies of these 
87 
early components could inform our understanding of speech processing if deployed in experiments that 
88 
compare conditions requiring different states of processing, such as attended/unattended speech or 
89 
understood/foreign language. For example, if a wave I from the auditory nerve differed between speech 
90 
stimuli that were attended versus unattended, then this would add to our current understanding of the 
91 
brainstem’s role in speech processing. Therefore, a click-like response that is evoked by speech stimuli 
92 
facilitates new investigations into speech encoding and processing.  
93 
Thus, we asked if we could further assess underlying speech encoding and processing in multiple 
94 
distinct early stages of the auditory system by 1) evoking additional waves than wave V of the canonical 
95 
ABR, and 2) measuring responses to different frequency ranges of speech (corresponding to different 
96 
places of origin on the cochlea). The ABR is strongest to very short stimuli such as clicks, so we created 
97 
“peaky” speech. The design goal of peaky speech is to re-synthesize natural speech so that its defining 
98 
spectrotemporal content is unaltered – maintaining the speech as intelligible and identifiable – but its 
99 
pressure waveform consists of maximally sharp peaks so that it drives the ABR as effectively as possible 
100 
(giving a very slight “buzzy” quality when listening under good headphones; Audio files 1–6). The results 
101 
show that peaky speech evokes canonical brainstem responses and frequency-specific responses, paving 
102 
the way for novel studies of subcortical contributions to speech processing. 
103 
 
104 
RESULTS  
105 
Broadband peaky speech yields more robust responses than unaltered speech 
106 
Broadband peaky speech elicits canonical brainstem responses 
107 
In previous work, brainstem responses to natural, on-going speech exhibited a temporally broad 
108 
wave V but no earlier waves (Maddox and Lee, 2018). We re-synthesized speech to be “peaky” with the 
109 
primary aim to evoke additional, earlier waves of the ABR that identify different neural generators. Indeed, 
110 
Figure 1 shows that waves I, III, and V of the canonical ABR are clearly visible in the group average and in 
111 
the individual responses to broadband peaky speech. This means that broadband peaky speech, unlike the 
112 
unaltered speech, can be used to assess naturalistic speech processing at discrete parts of the subcortical 
113 
auditory system, from the auditory nerve to rostral brainstem. These responses represent weighted 
114 
averaged data from ~43 minutes of continuous speech (40 epochs of 64 s each), and were filtered at a 
115 
typical high-pass cutoff of 150 Hz to highlight the earlier ABR waves. Responses containing an equal 
116 
number of epochs from the first and second half of the recording had a median (interquartile range) 
117 
correlation coefficient of 0.96 (0.95–0.98) for the 0–15 ms lags, indicating good replication. 
118 
Morphology of the broadband peaky speech ABR was inspected and waves marked by a trained 
119 
audiologist (MJP) on 2 occasions that were 3 months apart. The intraclass correlation coefficients for 
120 
absolute agreement (ICC3) were ≥ 0.90 (lowest ICC3 95% confidence interval was 0.79–0.95 for wave I, p 
121 
< 0.01), indicating excellent reliability for chosen peak latencies. Waves I and V were identifiable in 
122 
responses from all subjects (N = 22), and wave III was identifiable in 19 of the 22 subjects. The numbers of 
123 
subjects with identifiable waves I and III in these peaky speech responses were similar to the 24 and 16 out 
124 
of 24 subjects for the click-evoked responses in Maddox and Lee (2018). These waves are marked on the 
125 
individual responses in Figure 1. Mean ± SEM peak latencies for ABR waves I, III, and V were 3.23 ± 0.09 
126 
ms, 5.51 ± 0.07 ms, and 7.22 ± 0.07 ms respectively. These mean peak latencies are shown superimposed 
127 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
5 
on the group average response in Figure 1 (bottom right). Inter-wave latencies were 2.24 ± 0.06 ms (N = 
128 
19) for I–III, 1.68 ± 0.05 ms (N = 19) for III–V, and 4.00 ± 0.08 (N = 22) for I–V. These peak inter-wave 
129 
latencies fall within a range expected for brainstem responses but the absolute peak latencies were later 
130 
than those reported for a click ABR at a level of 60 dB sensation level (SL) and rate between 50 to 100 Hz 
131 
(Burkard and Hecox, 1983; Chiappa et al., 1979; Don et al., 1977). 
132 
 
133 
 
Figure 1. Single subject and group average (bottom right) weighted-average auditory brainstem responses (ABR) to ~43 
minutes of broadband peaky speech. Area for the group average shows ± 1 SEM. Responses were high-pass filtered at 150 
Hz using a first order Butterworth filter. Waves I, III, and V of the canonical ABR are evident in most of the single subject 
responses (N = 22, 16, 22 respectively), and are marked by the average peak latencies on the average response. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
6 
More components of the ABR and MLR are present with broadband peaky than unaltered speech 
134 
Having established that broadband peaky speech evokes robust canonical ABRs, we next 
135 
compared both ABR and MLR responses to those evoked by unaltered speech. To simultaneously 
136 
evaluate ABR and MLR components, a high-pass filter with a 30 Hz cutoff was used on the responses to 
137 
~43 minutes of each type of speech. Figure 2A shows that overall there were morphological similarities 
138 
between responses to both types of speech; however, there were more early and late component waves to 
139 
broadband peaky speech. More specifically, whereas both types of speech evoked waves V, Na and Pa, 
140 
broadband peaky speech also evoked waves I, often III (14–19 of 22 subjects depending if a 30 or 150 Hz 
141 
high-pass filter cutoff was used), and P0. With a lower cutoff for the high-pass filter, wave III rode on the 
142 
slope of wave V and was less identifiable in the grand average shown in Figure 2A than that shown with a 
143 
higher cutoff in Figure 1. Wave V was more robust and sharper to broadband peaky speech but peaked 
144 
slightly later than the broader wave V to unaltered speech. For reasons unknown to us, the half-rectified 
145 
speech method missed the MLR wave P0, and consequently had a broader and earlier Na than the 
146 
broadband peaky speech method, though this missing P0 was consistent with the results of Maddox and 
147 
Lee (2018). These waveforms indicate that broadband peaky speech is better than unaltered speech at 
148 
evoking canonical responses that distinguish activity from distinct subcortical and cortical neural 
149 
generators.  
150 
Peak latencies for the waves common to both types of speech are shown in Figure 2B. Again, there 
151 
was good agreement in peak wave choices for each type of speech, with ICC3 ≥ 0.95 (the lowest two ICC3 
152 
95% confidence intervals were 0.91–0.98 and 0.94–0.99 for waves V and Na to unaltered speech 
153 
respectively). As suggested by the waveforms in Figure 2A, mean ± SEM peak latencies for waves V, Na, 
154 
and Pa were longer for broadband peaky than unaltered speech by 0.87 ± 0.06 ms (independents t-test, t(21) 
155 
= 14.7 p < 0.01, d = 3.22), 6.92 ± 0.44 ms (t(21) = 15.4, p < 0.01, d = 3.44), and 1.07 ± 0.45 ms (t(20) = 2.3, p 
156 
= 0.032, d = 0.52) respectively.  
157 
The response to broadband peaky speech showed a small but consistent response at negative and 
158 
early positive lags (i.e., pre-stimulus) when using the pulse train as a regressor in the deconvolution, 
159 
particularly when using the lower high-pass filter cutoff of 30 Hz (Figure 2A) compared to 150 Hz (Figure 
160 
1). For a perfectly linear causal system this would not be expected. To better understand the source of this 
161 
 
Figure 2. Comparison of auditory brainstem (ABR) and middle latency responses (MLR) to ~43 minutes each of unaltered 
speech and broadband peaky speech. (A) The average waveform to broadband peaky speech (blue) shows additional, and 
sharper, waves of the canonical ABR and MLR than the broader average waveform to unaltered speech (black). Responses 
were high-pass filtered at 30 Hz with a first order Butterworth filter. Areas show ± 1 SEM. (B) Comparison of peak latencies for 
ABR wave V (circles) and MLR waves Na (downward triangles) and Pa (upward triangles) that were common between 
responses to broadband peaky and unaltered speech. Blue symbols depict individual subjects and black symbols depict the 
mean. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
7 
pre-stimulus component – and to determine whether later components were influencing the earliest 
162 
components – we completed two models: 1) a simple linear deconvolution model in which EEG for each 
163 
trial was simulated by convolving the rectified broadband peaky speech audio with an ABR kernel (the 
164 
average broadband peaky speech ABR was zero-padded to the beginning of wave I, filtered with a Hann 
165 
window and then normalized); and 2) a more nuanced and well-known model of the auditory nerve and 
166 
periphery that accounts for acoustics and some of the nonlinearities of the auditory system (Rudnicki et al., 
167 
2015; Verhulst et al., 2018; Zilany et al., 2014) – we simulated EEG for waves I, III and V using the 
168 
framework described by Verhulst et al. (2018), but due to computation constraints, modified the 
169 
implementation to use the peripheral model by Zilany et al. (2014). The simulated EEG of each model was 
170 
then deconvolved with the pulse trains to derive the modeled responses, which are shown in comparison 
171 
with the measured response in Figure 3. The pre-stimulus component of the measured response was 
172 
present in both models, suggesting that there are nonlinear parts of the response that are not accounted 
173 
for in the deconvolution with the pulse train regressor. However, the pre-stimulus component was 
174 
temporally broad compared to the components representing activity from the auditory nerve and cochlear 
175 
nucleus (i.e., waves I and III), and could thus be dealt with by high-pass filtering. The pre-stimulus 
176 
component was reduced with a 150 Hz first order Butterworth high-pass filter (Figure 1), and was 
177 
minimized with a more aggressive 200 Hz second order Butterworth high-pass filter (Figure 3, bottom). 
178 
Therefore, when doing an experiment where the analysis needs to evaluate specific contributions to the 
179 
earliest ABR components, we recommend high-pass filtering to help mitigate the complex and time-varying 
180 
nonlinearities inherent in the auditory system, as well as potential influences by responses from later 
181 
sources.  
182 
 
183 
We verified that the EEG data collected in response to broadband peaky speech could be 
184 
regressed with the half-wave rectified speech to generate a response. Figure 4A shows that the derived 
185 
responses to unaltered and broadband peaky speech were similar in morphology when using the half-wave 
186 
 
Figure 3. Comparison of grand average (N = 22) measured and modeled responses to ~43 minutes of broadband peaky 
speech. Amplitudes of the linear (dashed line) and auditory nerve (AN; dotted line) modelled responses were in arbitrary units, 
and thus scaled to match the amplitude of the measured response (solid line) over the 0–20 ms lags. The pre-stimulus 
component was present in all three responses using a first order 30 Hz high-pass Butterworth filter (top row), but was 
minimized by aggressive high-pass filtering with a second order 200 Hz high-pass Butterworth filter (bottom row). 
 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
8 
rectified audio as the regressor. Correlation coefficients from the 22 subjects for the 0–40 ms lags had a 
187 
median (interquartile range) of 0.84 (0.72–0.91), which were similar to correlation coefficients obtained by 
188 
re-analyzing the data split into even/odd epochs that each contained an equal number of epochs with EEG 
189 
to unaltered speech and peaky broadband speech (0.86, interquartile range 0.80–0.91; Wilcoxon signed-
190 
rank test W(21) = 89.0, p = 0.235). This means that the same EEG collected to broadband peaky speech 
191 
can be flexibly used to generate the robust canonical brainstem response to the pulse train, as well as the 
192 
broader response to the half-wave rectified speech. 
193 
Although responses to broadband peaky speech can be derived from both the half-wave rectified 
194 
audio and pulse train regressors, the same cannot be said about unaltered speech. We also regressed the 
195 
EEG data collected in response to unaltered speech with the pulse train. Figure 4B shows that simply using 
196 
the pulse train as a regressor does not give a robust canonical response – the response contained a wave 
197 
that slightly resembled wave V of the response to broadband peaky speech, albeit at a later latency and 
198 
smaller amplitude, but there were no other earlier waves of the ABR or later waves of the MLR. The 
199 
correlation coefficients comparing the unaltered and broadband peaky speech responses to the pulse train 
200 
(median 0.20, interquartile range 0.05–0.39) were significantly poorer than that of even/odd splits of the 
201 
same EEG (median 0.58, interquartile range 0.25–0.78; W(21) = 25.0, p < 0.001).The response morphology 
202 
was abnormal, with an acausal response at 0 ms and a smearing of the response in time, particularly at 
203 
lags of the MLR – these effects are consistent with some phases in the unaltered speech that come pre-
204 
pulse, which differs from that of the peaky speech which has aligned phases at each glottal pulse. The 
205 
aligned phases of the broadband peaky response allow for the distinct waves of the canonical brainstem 
206 
and middle latency responses to be derived using the pulse train regressor. 
207 
 
208 
 
209 
Broadband peaky speech responses differ across talkers 
210 
We next sought to determine whether response morphologies depended on the talker identity. 
211 
Responses derived from unaltered speech show similar, but not identical, morphology between male and 
212 
female narrators, indicating some dependence on the stimulus (Maddox and Lee, 2018). To determine 
213 
what extent the morphology and robustness of peaky speech responses depend on a specific narrator’s 
214 
voice and fundamental frequency – and therefore, rate of stimulation – we compared waveforms and peak 
215 
wave latencies for 32 minutes (30 epochs of 64 s each) each of male- and female-narrated broadband 
216 
 
Figure 4. Comparison of responses derived by using the same type of regressor in the deconvolution. Average waveforms 
(areas show ± 1 SEM) are shown for ~43 minutes each of unaltered speech (black) and broadband peaky speech (blue). EEG 
was regressed with the (A) half-wave rectified audio and (B) pulse train. Responses were high-pass filtered at 30 Hz using a 
first order Butterworth filter. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
9 
peaky speech in 11 subjects. The average fundamental frequency was 115 Hz for the male narrator and 
217 
198 Hz for the female narrator. 
218 
The group average waveforms to female- and male-narrated broadband peaky speech showed 
219 
similar canonical morphologies but were smaller and later for female-narrated ABR responses (Figure 5A), 
220 
much as they would be for click stimuli presented at higher rates (e.g., Burkard et al., 1990; Burkard and 
221 
Hecox, 1983; Chiappa et al., 1979; Don et al., 1977; Jiang et al., 2009). All component waves of the ABR 
222 
and MLR were visible in the group average, although fewer subjects exhibited a clear wave III in the 
223 
female-narrated response (9 versus all 11 subjects). The median (interquartile range) male-female 
224 
correlation coefficients were 0.68 (0.56–0.78) for ABR lags of 0–15 ms with a 150 Hz high-pass filter, and 
225 
0.53 (0.50–0.60) for ABR/MLR lags of 0–40 ms with a 30 Hz high-pass filter (Figure 5B). This stimulus 
226 
dependence was significantly different than the variability introduced by averaging only half the epochs 
227 
(i.e., splitting by male- and female-narrated epochs) – the correlation coefficients for the data reanalyzed 
228 
into even and odd epochs (each of the even/odd splits contained the same number of male- and female-
229 
narrated epochs), had a median (interquartile range) of 0.89 (0.79–0.95) for ABR lags and 0.66 (0.39–0.78) 
230 
for ABR/MLR lags. These odd-even coefficients were significantly higher than the male-female coefficients 
231 
for the ABR (W(10) = 0.0, p = 0.001; Wilcoxon signed-rank test) but not when the response included all lags 
232 
of the MLR (W(10) = 17.0, p = 0.175). which is indicative of the increased variability of these later waves. 
233 
These overall differences for the ABR indicate that the choice of narrator for using peaky speech impacts 
234 
the morphology of the early response. 
235 
As expected from the waveforms, peak latencies of component waves differed between male- and 
236 
female-narrated broadband peaky speech (Figure 5C). As before, ICC3 ≥ 0.90 indicated good agreement 
237 
in peak wave choices (the lowest two 95% confidence intervals were 0.79–0.95 for Na and 0.93–0.99 for I). 
238 
Mean ± SEM peak latency differences (female – male) for wave I, III, and V of the ABR were 0.21 ± 0.13 
239 
ms (t(10) = 1.59, p = 0.144, d = 0.50), 0.42 ± 0.11 ms (t(9) = 3.47, p = 0.007, d = 1.16), and 0.54 ± 0.09 ms 
240 
(t(10) = 5.56, p < 0.001, d =1.76) respectively. Latency differences were not significant for MLR peaks (P0: 
241 
−0.89 ± 0.40 ms, t(9) = −2.13, p = 0.062, d = −0.71; Na: −0.91 ± 0.55 ms, t(8) = −1.56, p = 0.158, d = −0.55; 
242 
Pa: 0.09 ± 0.55 ms, t(9) =−0.16, p = 0.880, d = −0.05). 
243 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
10 
 
244 
Multiband peaky speech yields frequency-specific brainstem responses to speech 
245 
Frequency-specific responses show frequency-specific lags 
246 
Broadband peaky speech gives new insights into subcortical processing of naturalistic speech. Not 
247 
only are brainstem responses used to evaluate processing at different stages of auditory processing, but 
248 
ABRs can also be used to assess hearing function across different frequencies. Traditionally, frequency-
249 
specific ABRs are measured using clicks with high-pass masking noise or frequency-specific tone pips. We 
250 
tested the flexibility of using our new peaky speech technique to investigate how speech processing differs 
251 
across frequency regions, such as 0–1, 1–2, 2–4, and 4–8 kHz frequency bands. To do this, we created 
252 
new pulse trains with slightly different fundamental waveforms for each filtered frequency regions of 
253 
speech, and then combined those filtered frequency bands together as multiband speech (for details, see 
254 
the Multiband peaky speech subsection of Methods). Using this method, we took advantage of the fact that 
255 
over time, stimuli with slightly different fundamental frequencies will be independent, yielding independent 
256 
auditory brainstem responses.  Therefore, the same EEG was regressed with each band’s pulse train to 
257 
derive the ABR and MLR to each frequency band. 
258 
Mean ± SEM responses from 22 subjects to the 4 frequency bands (0–1, 1–2, 2–4, and 4–8 kHz) of 
259 
~43 minutes of male-narrated multiband peaky speech are shown as colored waveforms with solid lines in 
260 
 
Figure 5. Comparison of responses to 32 minutes each of male (dark blue) and female (light blue) narrated re-synthesized 
broadband peaky speech. (A) Average waveforms across subjects (areas show ± 1 SEM) are shown for auditory brainstem 
response (ABR) time lags with high-pass filtering at 150 Hz (top), and both ABR and middle latency response (MLR) time lags 
with a lower high-pass filtering cutoff of 30 Hz (bottom). (B) Histograms of the correlation coefficients between responses 
evoked by male- and female-narrated broadband peaky speech during ABR (top) and ABR/MLR (bottom) time lags. Solid lines 
denote the median and dotted lines the inter-quartile range. (C) Comparison of ABR (top) and MLR (bottom) wave peak 
latencies for individual subjects (gray) and the group mean (black). ABR and MLR responses were similar to both types of 
input but are smaller for female-narrated speech, which has a higher glottal pulse rate. Peak latencies for female-evoked 
speech were delayed during ABR time lags but faster for early MLR time lags.  
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
11 
Figure 6A. A high-pass filter with a cutoff of 30 Hz was used. Each frequency band response comprises a 
261 
frequency-band-specific component as well as a band-independent common component, both of which are 
262 
due to spectral characteristics of the stimuli and neural activity. The pulse trains are independent over time 
263 
in the vocal frequency range – thereby allowing us to pull out responses to each different pulse train and 
264 
frequency band from the same EEG – but they became coherent at frequencies lower than 72 Hz for the 
265 
male-narrated speech and 126 Hz for the female speech (see Figure 15 in Methods). This coherence was 
266 
due to all pulse trains beginning and ending together at the onset and offset of voiced segments and was 
267 
the source of the low-frequency common component of each band’s response. The way to remove the 
268 
common component is to calculate the common activity across the frequency band responses and subtract 
269 
this waveform from each of the frequency band responses. This common component was calculated by 
270 
regressing the EEG to multiband speech with 6 independent “fake” pulses trains – pulse trains with slightly 
271 
different fundamental frequencies that were not used to create the multiband peaky speech stimuli that 
272 
were presented during the experiment – and then averaging across these 6 responses. This common 
273 
component waveform is shown by the dot-dashed gray line, which is superimposed with each response to 
274 
the frequency bands in Figure 6A. The subtracted, frequency-specific waveforms to each frequency band 
275 
are shown by the solid lines in Figure 6B. Of course, the subtracted waveforms could also then be high-
276 
pass filtered at 150 Hz to highlight earlier waves of the brainstem responses, as shown by the dashed lines 
277 
in Figure 6B. However, this method reduces the amplitude of the responses, which in turn affects response 
278 
SNR and detectability. In some scenarios, due to the low-pass nature of the common component, high-
279 
passing the waveforms at a high enough frequency may obviate the need to formally subtract the common 
280 
component. For example, at least for the narrators used in these experiments, the common component 
281 
contained minimal energy above 150 Hz, so if the waveforms are already high-passed at 150 Hz to focus 
282 
on the early waves of the ABR, then the step of formally subtracting the common component may not be 
283 
necessary. But beyond the computational cost, there is no reason not to subtract the common component, 
284 
and doing so allows lower filter cut-offs to be used. 
285 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
12 
 
286 
 Overall, the frequency-specific responses showed characteristic ABR and MLR waves with longer 
287 
latencies for lower frequency bands, as would be expected from responses arising from different cochlear 
288 
regions. Also, waves I and III of the ABR were visible in the group average waveforms of the 2–4 kHz 
289 
(≥41% of subjects) and 4–8 kHz (≥86% of subjects) bands, whereas the MLR waves were more prominent 
290 
in the 0–1 kHz (≥95% of subjects) and 1–2 kHz (≥54% of subjects) bands. 
291 
 
Figure 6. Comparison of responses to ~43 minutes of male-narrated multiband peaky speech. (A) Average waveforms across 
subjects (areas show ± 1 SEM) are shown for each band (colored solid lines) and for the common component (dot-dash gray 
line, same waveform replicated as a reference for each band), which was calculated using 6 false pulse trains. (B) The 
common component was subtracted from each band’s response to give the frequency-specific waveforms (areas show ± 1 
SEM), which are shown with high-pass filtering at 30 Hz (solid lines) and 150 Hz (dashed lines). (C) Mean ± SEM peak 
latencies for each wave decreased with increasing band frequency. Numbers of subjects with an identifiable wave are given 
for each wave and band. Details of the mixed effects models for (C) are provided in Supplementary File 1A.  
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
13 
These frequency-dependent latency changes for the frequency-specific responses are highlighted 
292 
further in Figure 6C, which shows mean ± SEM peak latencies and the number of subjects who had a 
293 
clearly identifiable wave. ICC3 ≥ 0.89 indicated good agreement in peak wave choices (lowest two 95% 
294 
confidence intervals were 0.84–0.93 for Pa and 0.90–0.95 for Na). The nonlinear change in peak latency, 𝜏,  
295 
with frequency band was modeled using power law regression according to the formula (Harte et al., 2009; 
296 
Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009): 
297 
𝜏(𝑓) =  𝑎+ 𝑏𝑓−𝑑 
298 
where 
299 
𝑎= 𝜏𝑠𝑦𝑛𝑎𝑝𝑡𝑖𝑐+ 𝜏𝐼−𝑉, 
300 
and where 𝑓 is the band center frequency normalized to 1 kHz (i.e., divided by 1000), 𝜏𝑠𝑦𝑛𝑎𝑝𝑡𝑖𝑐 is the 
301 
synaptic delay (assumed to be 0.8; Eggermont, 1979; Strelcyk et al., 2009), and 𝜏𝐼−𝑉 is the I-V inter-wave 
302 
delay from the subjects' responses to broadband peaky speech. The power law model was completed for 
303 
each filter cutoff of 30 and 150 Hz in the log-log domain, according to the formula: 
304 
log10(𝜏(𝑓) −𝑎) = log10 𝑏+ (−𝑑) log10 𝑓 
305 
and using linear mixed effects regression to estimate the terms 𝑑 and 𝑏 (calculated as 𝑏 = 10intercept). The 
306 
fixed effect of wave and its interaction with frequency were added to the model, along with random effects 
307 
of intercept and frequency for each subject. For the 30 and 150 Hz filter cutoffs, the mean ± SEM 𝜏𝐼−𝑉 was 
308 
4.33 ± 0.08 and 4.00 ± 0.08 respectively, resulting in average estimates of 5.13 and 4.80 for 𝑎 respectively 
309 
– both of which are similar to the post-cochlear delays of 4.7–5.0 ms reported for tone pips and derived-
310 
bands from clicks (Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009). There were 
311 
insufficient numbers of subjects with identifiable waves I and III for the 0–1 kHz and 1–2 kHz bands, so 
312 
these waves were not included in the full model. Details of each log-log mixed effects model are described 
313 
in Supplementary File 1A. For wave V, the estimated mean parameters were 𝑏 = 3.95 ms and  𝑑 = -0.41 for 
314 
a 30 Hz filter cutoff, and 𝑏 = 3.95 ms and  𝑑 = -0.37 for a 150 Hz filter cutoff, which also fell within the 
315 
corresponding ranges of 3.46–5.39 ms for 𝑏 (calculated for a level of 65 dB ppeSPL using the models’ level 
316 
dependent parameter) and 0.22–0.50 for −𝑑 that were previously reported for tone pips and derived-bands 
317 
(Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009). As expected, there were significantly 
318 
different latencies for each MLR wave P0, Na and Pa compared to the ABR wave V (all effects of wave on 
319 
the intercept p < 0.001, for each high-pass filter cutoff of 30 and 150 Hz). The significant decrease in 
320 
latency with frequency band (slope, 𝑑 : p < 0.001 for 30 and 150 Hz) was shallower (i.e., less negative) for 
321 
MLR waves compared to the ABR wave V (all p < 0.001 for interactions between wave and frequency term 
322 
for 30 and 150 Hz).  
323 
Next, the frequency-specific responses (i.e., multiband responses with common component 
324 
subtracted) were summed and the common component added to derive the entire response to multiband 
325 
peaky speech. As shown in Figure 7, this summed multiband response was strikingly similar in morphology 
326 
to the broadband peaky speech. Both responses were high-passed filtered at 150 Hz and 30 Hz to 
327 
highlight the earlier ABR waves and later MLR waves, respectively. The median (interquartile range) 
328 
correlation coefficients from the 22 subjects were 0.90 (0.84–0.94) for 0–15 ms ABR lags, and 0.66 (0.55–
329 
0.83) for 0–40 ms MLR lags. The similarity verifies that the frequency-dependent responses are 
330 
complementary to each other to the common component, such that these components add linearly into a 
331 
“whole” broadband response. If there were significant overlap in the cochlear regions, for example, the 
332 
summed response would not resemble the broadband response to such a degree, and would instead be 
333 
larger. The similarity also verified that the additional changes we made to create re-synthesized multiband 
334 
peaky speech did not significantly affect responses compared to broadband peaky speech. 
335 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
14 
 
336 
We also verified that the EEG data collected in response to multiband peaky speech could be 
337 
regressed with the half-wave rectified speech to generate a response.  Figure 4-figure supplement 1 shows 
338 
that the derived responses to unaltered and multiband peaky speech were similar in morphology when 
339 
using the half-wave audio as the regressor, although the multiband peaky speech response was broader in 
340 
the earlier latencies. Correlation coefficients from the 22 subjects for the 0–40 ms lags had a median 
341 
(interquartile range) of 0.83 (0.77–0.88), which were slightly smaller than the correlation coefficients 
342 
obtained by re-analyzing the data split into even/odd epochs that each contained an equal number of 
343 
epochs with EEG to unaltered speech and peaky broadband speech (0.89, interquartile range 0.83–0.94; 
344 
Wilcoxon signed-rank test W(21) = 58.0, p = 0.025). This means that the same EEG collected to multiband 
345 
peaky speech can be flexibly used to generate the frequency-specific brainstem responses to the pulse 
346 
train, as well as the broader response to the half-wave rectified speech. 
347 
 
348 
Frequency-specific responses also differ by narrator 
349 
We also investigated the effects of narrator on multiband peaky speech by deriving responses to 32 
350 
minutes (30 epochs of 64 s each) each of male- and female-narrated multiband peaky speech in the same 
351 
11 subjects. As with broadband peaky speech, responses to both narrators showed similar morphology, 
352 
but the responses were smaller and the MLR waves more variable for the female than male narrator 
353 
(Figure 8A). Figure 8B shows the male-female correlation coefficients for responses between 0–40 ms with 
354 
a high-pass filter of 30 Hz and between 0–15 ms with a high-pass filter of 150 Hz. The median (interquartile 
355 
range) male-female correlation coefficients were better for higher frequency bands, ranging from 0.12 
356 
(−0.11–0.34) for the 1–2 kHz band to 0.44 (0.32–0.49) for the 4–8 kHz band for MLR lags (Figure 8B, left 
357 
panel), and from 0.49 (0.30–0.69) for the 1–2 kHz band to 0.79 (0.64–0.81) for the 4–8 kHz band for ABR 
358 
lags (Figure 8B, right panel). These male-female correlation coefficients were significantly weaker than 
359 
those of the same EEG split into even and odd trials for all but the 2–4 kHz frequency band when 
360 
responses were high-pass filtered at 30 Hz and correlated across 0–40 ms lags (2–4 kHz: W(10) = 17.0, p = 
361 
0.175; other bands: W(10) ≤ 5.0, p ≤ 0.010), but were similar to the even/odd trials for responses from all 
362 
frequency bands high-pass filtered at 150 Hz (W(10) ≥ 11.0, p ≥ 0.054). These results indicate that the 
363 
specific narrator can affect the robustness of frequency-specific responses, particularly for the MLR waves.  
364 
 
Figure 7. Comparison of responses to ~43 minutes of male-narrated peaky speech in the same subjects. Average waveforms 
across subjects (areas show ± 1 SEM) are shown for broadband peaky speech (blue) and for the summed frequency-specific 
responses to multiband peaky speech with the common component added (red), high-pass filtered at 150 Hz (left) and 30 Hz 
(right). Regressors in the deconvolution were pulse trains.  
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
15 
 
365 
 
Figure 8. Comparison of responses to 32 minutes each of male- and female-narrated re-synthesized multiband peaky speech. 
(A) Average frequency-specific waveforms across subjects (areas show ± 1 SEM; common component removed) are shown 
for each band in response to male- (dark red lines) and female-narrated (light red lines) speech. Responses were high-pass 
filtered at 30 Hz (left) and 150 Hz (right) to highlight the MLR and ABR respectively. (B) Correlation coefficients between 
responses evoked by male- and female-narrated multiband peaky speech during ABR/MLR (left) and ABR (right) time lags for 
each frequency band. Black lines denote the median. (C) Mean ± SEM peak latencies for male- (dark) and female- (light) 
narrated speech for each wave decreased with increasing frequency band. Numbers of subjects with an identifiable wave are 
given for each wave, band and narrator. Lines are given a slight horizontal offset to make the error bars easier to see. Details 
of the mixed effects models for (C) are provided in Supplementary File 1B. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
16 
As expected from the grand average waveforms and male-female correlations, there were fewer 
366 
subjects who had identifiable waves across frequency bands for the female- than male-narrated speech. 
367 
These numbers are shown in Figure 8C, along with the mean ± SEM peak latencies for each wave, 
368 
frequency band and narrator. ICC3 ≥0.91 indicated good agreement in peak latency choices (the two 
369 
lowest 95% confidence intervals were 0.82–0.96 for wave III and 0.98–0.99 for Pa, both with a high-pass 
370 
filter cutoff of 30 Hz). Again, there were few numbers of subjects with identifiable waves I and III for the 
371 
lower frequency bands. Therefore, a power law model (see previous subsection for the formula) was 
372 
completed for waves V, P0, Na, and Pa of responses in the 4 frequency bands that were high-pass filtered at 
373 
30 Hz. The model was completed in the log-log domain with linear mixed effect regression to estimate the 
374 
terms 𝑑 and 𝑏 (calculated as 𝑏 = 10intercept). The mixed effects model included fixed effects of narrator, 
375 
wave, logged center frequency that was normalized to 1 kHz, the interaction between narrator and wave 
376 
and between narrator and frequency, and the interactions between wave and frequency, as well as a 
377 
random intercept and frequency term per subject. The mean ± SEM 𝜏𝐼−𝑉 for the male and female narrators 
378 
was 4.26 ± 0.14 and 4.78 ± 0.12 respectively, resulting in average estimates of 5.06 and 5.58 for 𝑎 
379 
respectively. The value of 𝑎 for the male narrator was similar to that obtained in the first cohort of 22 
380 
subjects and was consistent with previously reported values for tone pips and derived-bands from clicks 
381 
(Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009). The 𝑎 estimate for the female narrator 
382 
was slightly longer than the previously reported values, although a lower filter cut-off of 30 Hz was used 
383 
than the 100 Hz in the previous studies. Filtering the female-narrated responses at 150 Hz gave an 
384 
estimate for 𝑎 of 5.18, which is closer to the corresponding range of 4.7–5.0 ms than 5.58. Details of the 
385 
log-log mixed effects model are described in Supplementary File 1B. For wave V, the estimated mean 
386 
parameters were 𝑏 = 4.42 ms and 𝑑 = -0.45 for the male narrator, and 𝑏 = 3.94 ms and  𝑑 = -0.44 for the 
387 
female narrator. The 𝑏, but not 𝑑, parameter was significantly different between narrators (narrator main 
388 
effect, p = 0.001; narrator interaction with frequency: p = 0.732), and the parameters were within the 
389 
ranges previously reported for tone pips and derived-bands at 65 dB ppeSPL (Neely et al., 1988; 
390 
Rasetshwane et al., 2013; Strelcyk et al., 2009).  For those subjects with identifiable waves, peak latencies 
391 
shown in Figure 8C differed by wave (p < 0.001 for effects of each wave on the intercept), and latency 
392 
decreased with increasing frequency (slope, 𝑑: p < 0.001). This change with frequency was smaller (i.e., 
393 
shallower slope) for each MLR wave compared to wave V (p < 0.001 for all interactions between wave and 
394 
the term for frequency band). There was a main effect of narrator on peak latencies but no interaction with 
395 
wave (narrator p = 0.001,  wave-narrator interactions p > 0.087). Therefore, as with broadband peaky 
396 
speech, frequency-specific peaky responses were more robust with the male narrator and the frequency-
397 
specific responses peaked earlier for a narrator with a lower fundamental frequency. 
398 
 
399 
Frequency-specific responses can be measured simultaneously in each ear (dichotically) 
400 
We have so far demonstrated the effectiveness of peaky speech for diotic stimuli, but there is often 
401 
a need to evaluate auditory function in each ear, and the most efficient tests assess both ears 
402 
simultaneously. Applying this principle to generate multiband peaky speech, we investigated whether ear-
403 
specific responses could be evoked across the 5 standard audiological octave frequency bands (500–8000 
404 
Hz) using dichotic multiband speech. We created 10 independent pulse trains, 2 for each ear in each of the 
405 
5 frequency bands (see Multiband peaky speech and Band filters in Methods). 
406 
We recorded responses to 64 minutes (60 epochs of 64 s each) each of male- and female-narrated 
407 
dichotic multiband peaky speech in 11 subjects. The frequency-specific (i.e., common component-
408 
subtracted) group average waveforms for each ear and frequency band are shown in Figure 9A. The ten 
409 
waveforms were small, especially for female-narrated speech, but a wave V was identifiable for both 
410 
narrators. Also, waves I and III of the ABR were visible in the group average waveforms of the 4 kHz band 
411 
(≥45% and 18% of subjects for the male and female narrator respectively) and 8 kHz band (≥90% and 72% 
412 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
17 
of subjects for the male and female narrator respectively). MLR waves were not clearly identifiable for 
413 
responses to female-narrated speech. Therefore, correlations between responses were performed for ABR 
414 
lags between 0–15 ms. As shown in Figure 9B, the median (interquartile range) left-right ear correlation 
415 
coefficients (averaged across narrators) ranged from 0.28 (0.02–0.52) for the 0.5 kHz band to 0.73 (0.62–
416 
0.84) for the 8 kHz band. Male-female correlation coefficients (averaged across ear) ranged from 0.44 
417 
(0.00–0.58) for the 0.5 kHz band to 0.76 (0.51–0.80) for the 8 kHz band. Although the female-narrated 
418 
responses were smaller than the male-narrated responses, these male-female coefficients did not 
419 
significantly differ from correlations of same EEG split into even-odd trials and averaged across ear (W(10) ≥ 
420 
12.0, p ≥ 0.067), likely reflecting the variability in such small responses. 
421 
Figure 9C shows the mean ± SEM peak latencies of wave V for each ear and frequency band for 
422 
the male- and female-narrated dichotic multiband peaky speech. The ICC3 for wave V was 0.97 (95% 
423 
confidence interval 0.97–0.98), indicating reliable peak latency choices. The nonlinear change in wave V 
424 
latency with frequency was modeled by a power law using log-log mixed effects regression with fixed 
425 
effects of narrator, ear, logged band center frequency normalized to 1 kHz, and the interactions between 
426 
narrator and frequency. Random effects included an intercept and frequency term for each subject. The 
427 
experiment 2 average estimates for 𝑎 of 5.06 and 5.58 for the male- and female-narrated responses were 
428 
used for each subject. Details of the model are described in Supplementary File 1C. For wave V, the 
429 
estimated mean parameters were 𝑏 = 4.13 ms (calculated as 10intercept) and  𝑑 = -0.36 for the male narrator, 
430 
and 𝑏 = 4.25 ms and  𝑑 = -0.41 for the female narrator, which corresponded to previously reported ranges 
431 
for tone pips and derived-bands at 65 dB ppeSPL (Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et 
432 
al., 2009). Latency decreased with increasing frequency (slope, 𝑑, p < 0.001) but did not differ between 
433 
ears (p = 0.265). The  𝑏 parameter differed by narrator but the slope did not change with narrator 
434 
(interaction with intercept p = 0.004, interaction with slope p = 0.085). Taken together, these results confirm 
435 
that, while small in amplitude, frequency-specific responses can be elicited in both ears across 5 different 
436 
frequency bands and show characteristic latency changes across the different frequency bands. 
437 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
18 
 
438 
 
439 
Responses are obtained quickly for male-narrated broadband peaky speech but not multiband speech 
440 
Having demonstrated that peaky broadband and multiband speech provides canonical waveforms 
441 
with characteristic changes in latency with frequency, we next evaluated the acquisition time required for 
442 
waveforms to reach a decent SNR. We chose 0 dB SNR based on visual assessment of when waveforms 
443 
were easily inspected and based on what we have done previously (Maddox and Lee, 2018; Polonenko 
444 
 
Figure 9. Comparison of responses to ~60 minutes each of male- and female-narrated dichotic multiband peaky speech with 
standard audiological frequency bands. (A) Average frequency-specific waveforms across subjects (areas show ± 1 SEM; 
common component removed) are shown for each band for the left ear (dotted lines) and right ear (solid lines). Responses 
were high-pass filtered at 30 Hz. (B) Left-right ear correlation coefficients (top, averaged across gender) and male-female 
correlation coefficients (bottom, averaged across ear) during ABR time lags (0–15 ms) for each frequency band. Black lines 
denote the median. (C) Mean ± SEM wave V latencies for male- (dark red) and female-narrated (light red) speech for the left 
(dotted line, cross symbol) and right ear (solid line, circle symbol) decreased with increasing frequency band. Lines are given a 
slight horizontal offset to make the error bars easier to see. Details of the mixed effects model for (C) are provided in 
Supplementary File 1C. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
19 
and Maddox, 2019). SNR was calculated by comparing the variance in the MLR time interval 0–30 ms (for 
445 
responses high-pass filtered at 30 Hz) or ABR time interval 0–15 ms (for responses high-pass filtered at 
446 
150 Hz) to the variance in the pre-stimulus noise interval −480 to −20 ms (see Response SNR calculation 
447 
in Methods for details). 
448 
Figure 10 shows the cumulative proportion of subjects who had responses with ≥ 0 dB SNR to 
449 
unaltered and broadband peaky speech as a function of recording time. Acquisition times for 22 subjects 
450 
were similar for responses to both unaltered and broadband peaky male-narrated speech, with 0 dB SNR 
451 
achieved by 7–8 minutes in 50% of subjects and about 20 minutes for all subjects. For responses high-
452 
pass filtered at 150 Hz to highlight the ABR (0–15 ms interval), the time reduced to 2 and 5 minutes for 
453 
50% and 100% of subjects respectively for broadband peaky speech but increased to 8 and >20 minutes 
454 
for 50% and 100% of subjects respectively for unaltered speech. The increased time for the unaltered 
455 
speech reflects the broad morphology of the response during ABR lags. These times for male-narrated 
456 
broadband peaky speech were confirmed in our second cohort of 11 subjects. However, acquisition times 
457 
were at least 2.1 times – but in some cases over 10 times – longer for female-narrated broadband peaky 
458 
speech. In contrast to male-narrated speech, not all subjects achieved this threshold for female-narrated 
459 
speech by the end of the 32-minute recording. Taken together, these acquisition times confirm that 
460 
responses with useful SNRs can be measured quickly for male-narrated broadband peaky speech but 
461 
longer recording sessions are necessary for narrators with higher fundamental frequencies. 
462 
 
463 
The longer recording times necessary for a female narrator became more pronounced for the 
464 
multiband peaky speech. Figure 11A shows the cumulative density function for responses high-pass 
465 
filtered at 150 Hz and the SNR estimated over the ABR interval. Many subjects (72%) had frequency-
466 
specific responses (common component subtracted) with ≥ 0 dB SNR for all 4 frequency bands by the end 
467 
of the 32-minute recording for the male-narrated speech, but this was achieved in only 45% of subjects for 
468 
the female-narrated speech. Multiband peaky speech required significantly longer recording times than 
469 
broadband peaky speech, with 50% of subjects achieving 0 dB SNR by 15 minutes compared to 2 minutes 
470 
for the male-narrated responses across the ABR 0–15 ms interval and 17 minutes compared to 5 minutes 
471 
for the MLR 0–30 ms interval. Even more time was required for dichotic multiband speech, which was 
472 
comprised of a larger number of frequency bands (Figure 11B). All 10 audiological band responses 
473 
 
Figure 10. Cumulative proportion of subjects who have responses with ≥ 0 dB SNR as a function of recording time. Time 
required for unaltered (black) and broadband peaky speech (dark blue) of a male narrator are shown for 22 subjects in the left 
plot, and for male (dark blue) and female (light blue) broadband peaky speech is shown for 11 subjects in the right plot. Solid 
lines denote SNRs calculated using variance of the signal high-pass filtered at 30 Hz over the ABR/MLR interval 0–30 ms, and 
dashed lines denote SNR variances calculated on signals high-pass filtered at 150 Hz over the ABR interval 0–15 ms. Noise 
variance was calculated in the pre-stimulus interval −480 to −20 ms. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
20 
achieved ≥0 dB SNR in 45% of ears (10 / 22 ears from 11 subjects) by 64 minutes for male-narrated 
474 
speech and in 27% of ears (6 / 22 ears) for female-narrated speech. The smaller and broader responses in 
475 
the low frequency bands were slower to obtain and were the main constraint on testing time. These 
476 
recording times suggest that deriving multiple frequency-specific responses will require at least more than 
477 
30 minutes per condition for < 5 bands, and more than an hour session for one condition of peaky 
478 
multiband speech with 10 bands. 
479 
 
480 
 
 
481 
 
Figure 11. Cumulative proportion of subjects who have frequency-specific responses (common component subtracted) with ≥ 
0 dB SNR as a function of recording time. Acquisition time was faster for male (left) than female (right) narrated multiband 
peaky speech with (A) 4 frequency bands presented diotically, and with (B) 5 frequency bands presented dichotically (total of 
10 responses, 5 bands in each ear). SNR was calculated by comparing variance of signals high-pass filtered at 150 Hz across 
the ABR interval of 0–15 ms to variance of noise in the pre-stimulus interval −480 to −20 ms. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
21 
DISCUSSION 
482 
The major goal of this work was to develop a method to investigate early stages of naturalistic 
483 
speech processing. We re-synthesized continuous speech taken from audio books so that the phases of all 
484 
harmonics aligned at each glottal pulse during voiced segments, thereby making speech as impulse-like 
485 
(peaky) as possible to drive the auditory brainstem. Then we used the glottal pulse trains as the regressor 
486 
in deconvolution to derive the responses. Indeed, comparing waveforms to broadband peaky and unaltered 
487 
speech validated the superior ability of peaky speech to evoke additional waves of the canonical ABR and 
488 
MLR, reflecting neural activity from multiple subcortical structures. Robust ABR and MLR responses were 
489 
recorded in less than 5 and 20 minutes respectively for all subjects, with half of the subjects exhibiting a 
490 
strong ABR within 2 minute and MLR within 8 minutes. Longer recording times were required for the 
491 
smaller responses generated by a narrator with a higher fundamental frequency. We also demonstrated 
492 
the flexibility of this stimulus paradigm by simultaneously recording up to 10 frequency-specific responses 
493 
to multiband peaky speech that was presented either diotically or dichotically, although these responses 
494 
required much longer recording times. Taken together, our results show that peaky speech effectively 
495 
yields responses from distinct subcortical structures and from different frequency bands, paving the way for 
496 
new investigations of speech processing and new tools for clinical application. 
497 
 
498 
Peaky speech responses reflect activity from distinct subcortical components 
499 
Canonical responses can be derived from speech with impulse-like characteristics 
500 
For the purpose of investigating responses from different subcortical structures, we accomplished our 
501 
goal of creating a stimulus paradigm that overcame some of the limitations of current methods using natural 
502 
speech. Methods that do not use re-synthesized impulse-like speech generate responses characterized by 
503 
a broad peak between 6–9 ms  (Forte et al., 2017; Maddox and Lee, 2018), with contributions predominantly 
504 
from the inferior colliculus (Saiz-Alia and Reichenbach, 2020). In contrast, for the majority of our subjects, 
505 
peaky speech evoked responses with canonical morphology comprised of waves I, III, V, P0, Na, Pa (Figure 
506 
1), reflecting neural activity from distinct stages of the auditory system from the auditory nerve to thalamus 
507 
and primary auditory cortex (e.g., Picton et al., 1974). Although clicks also evoke responses with multiple 
508 
component waves, current studies of synaptopathy show quite varied results with clicks and poor correlation 
509 
with speech in noise (Bramhall et al., 2019; Prendergast et al., 2017). Obtaining click-like responses to stimuli 
510 
with all of speech’s spectrotemporal richness may provide a better connection to the specific neural 
511 
underpinnings of speech encoding, similar to how the complex cross-correlation to a fundamental waveform 
512 
can change based on the context of attention (Forte et al., 2017; Saiz-Alía et al., 2019).  
513 
The same ABR waves evoked here were also evoked by a method using embedded chirps intermixed 
514 
within alternating octave bands of speech, particularly if presented monaurally over headphones instead of 
515 
in free field (Backer et al., 2019; Miller et al., 2017). Chirps are transients that compensate for the cochlear 
516 
traveling delay wave by introducing different phases across frequency, leading to a more synchronized 
517 
response across the cochlea and a larger brainstem response than for clicks (Dau et al., 2000; Elberling and 
518 
Don, 2008; Shore and Nuttall, 1985). The responses to embedded chirps elicited waves with larger mean 
519 
amplitude than those to our broadband peaky speech (~0.4 versus ~0.2 μV, respectively), although a similar 
520 
proportion of subjects had identifiable waves, the SNR was good, and several other factors may contribute 
521 
to amplitude differences. For example, higher click rates (e.g., Burkard et al., 1990; Burkard and Hecox, 
522 
1983; Chiappa et al., 1979; Don et al., 1977; Jiang et al., 2009) and higher fundamental frequencies (Maddox 
523 
and Lee, 2018; Saiz-Alía et al., 2019; Saiz-Alia and Reichenbach, 2020) reduce the brainstem response 
524 
amplitude, and dynamic changes in rate may create interactions across neural populations that lead to 
525 
smaller amplitudes. Our stimuli kept the dynamic changes in pitch across all frequencies (instead of alternate 
526 
octave bands of chirps and speech) and created impulses at every glottal pulse, with an average pitch of 
527 
~115 Hz and ~198 Hz for the male and female narrators respectively. These presentation rates were much 
528 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
22 
higher and more variable than the flat 42 Hz rate at which the embedded chirps were presented (pitch 
529 
flattened to 82 Hz and chirps presented every other glottal pulse). We could evaluate whether chirps would 
530 
improve response amplitude to our dynamic peaky speech by simply all-pass filtering the re-synthesized 
531 
voiced segments by convolving with a chirp prior to mixing the re-synthesized parts with the unvoiced 
532 
segments. While maintaining the amplitude spectrum of speech, the harmonics would then have the different 
533 
phases associated with chirps at each glottal pulse instead of all phases set to 0. Regardless, our peaky 
534 
speech generated robust canonical responses with good SNR while maintaining a natural-sounding, if very 
535 
slightly “buzzy,” quality to the speech. Overall, continuous speech re-synthesized to contain impulse-like 
536 
characteristics is an effective way to elicit responses that distinguish contributions from different subcortical 
537 
structures. 
538 
Latencies of component waves are consistent with distinct subcortical structures 
539 
The latencies of the component waves of the responses to peaky speech are consistent with 
540 
activity arising from known subcortical structures. The inter-wave latencies between I-III, III-V and I-V fall 
541 
within the expected range for brainstem responses elicited by transients at 50–60 dB sensation level (SL) 
542 
and 50–100 Hz rates (Burkard and Hecox, 1983; Chiappa et al., 1979; Don et al., 1977), suggesting the 
543 
transmission times between auditory nerve, cochlear nucleus and rostral brainstem remain similar for 
544 
speech stimuli. However, these speech-evoked waves peak at later absolute latencies than responses to 
545 
transient stimuli at 60 dB SL and 90–100 Hz, but at latencies more similar to those presented at 50 dB SL 
546 
or 50 dB nHL in the presence of some masking noise (Backer et al., 2019; Burkard and Hecox, 1983; 
547 
Chiappa et al., 1979; Don et al., 1977; Maddox and Lee, 2018; Miller et al., 2017). There are reasons why 
548 
the speech-evoked latencies may be later. First, our level of 65 dB sound pressure level (SPL) may be 
549 
more similar to click levels of 50 dB SL. Second, although spectra of both speech and transients are broad, 
550 
clicks, chirps and even our previous speech stimuli (which was high-pass filtered at 1 kHz; Maddox and 
551 
Lee, 2018) have relatively greater high-frequency energy than the unaltered and peaky broadband speech 
552 
used in the present work.  Neurons with higher characteristic frequencies respond earlier due to their basal 
553 
cochlear location, and contribute relatively more to brainstem responses (e.g., Abdala and Folsom, 1995), 
554 
leading to quicker latencies for stimuli that have greater high frequency energy. Also consistent with having 
555 
greater lower frequency energy, our unaltered and peaky speech responses were later than the response 
556 
from the same speech segments that were high-pass filtered at 1 kHz (Maddox and Lee, 2018). In fact, the 
557 
ABR to broadband peaky speech bore a close resemblance to the summation of each frequency-specific 
558 
response and the common component to peaky multiband speech (Figure 7), with peak wave latencies 
559 
representing the relative contribution of each frequency band. Third, higher stimulation rates prolong 
560 
latencies due to neural adaptation, and the 115–198 Hz average fundamental frequencies of our speech 
561 
were much higher than the 41 Hz embedded chirps and 50–100 Hz click rates (e.g., Burkard et al., 1990; 
562 
Burkard and Hecox, 1983; Chiappa et al., 1979; Don et al., 1977; Jiang et al., 2009). The effect of 
563 
stimulation rate was also demonstrated by the later ABR wave I, III, and V peak latencies for the female 
564 
narrator with the higher average fundamental frequency of 198 Hz (Figure 6A&C). Therefore, the differing 
565 
characteristics of typical periodic transients (such as clicks and chirps) and continuous speech may give 
566 
rise to differences in brainstem responses, even though they share canonical waveforms arising from 
567 
similar contributing subcortical structures. 
568 
The latency of the peaky speech-evoked response also differed from the non-standard, broad 
569 
responses to unaltered speech. However, latencies from these waveforms are difficult to compare due to 
570 
the differing morphology and the different analyses that were used to derive the responses. Evidence for 
571 
the effect of analysis comes from the fact that the same EEG collected in response to peaky speech could 
572 
be regressed with pulse trains to give canonical ABRs (Figures 1, 2), or regressed with the half-wave 
573 
rectified peaky speech to give the different, broad waveform (Figure 4). Furthermore, non-peaky 
574 
continuous speech stimuli with similar ranges of fundamental frequencies (between 100–300 Hz) evoke 
575 
non-standard, broad brainstem responses that also differ in morphology and latency depending on whether 
576 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
23 
the EEG is analyzed by deconvolution with the half-wave rectified speech (Figure 2, Maddox and Lee, 
577 
2018) or complex cross-correlation with the fundamental frequency waveform (Forte et al., 2017). 
578 
Therefore, again, even though the inferior colliculus and lateral lemniscus may contribute to generating 
579 
these different responses (Møller and Jannetta, 1983; Saiz-Alia and Reichenbach, 2020; Starr and 
580 
Hamilton, 1976), the morphology and latency may differ (sometimes substantially) depending on the 
581 
analysis technique used. 
582 
Responses reflect activity from different frequency regions 
583 
In addition to evoking canonical brainstem responses, peaky speech can be exploited for other 
584 
traditional uses of ABR, such as investigating subcortical responses across different frequencies. 
585 
Frequency-specific responses were measurable to two different types of multiband peaky speech: 4 
586 
frequency bands presented diotically (Figures 6, 8), and 5 frequency bands presented dichotically (Figure 
587 
9). Peak wave latencies of these responses decreased with increasing band frequency in a similar way to 
588 
responses evoked by tone pips and derived-bands from clicks in noise (Gorga et al., 1988; Neely et al., 
589 
1988; Rasetshwane et al., 2013; Strelcyk et al., 2009), thereby representing activity evoked from different 
590 
areas across the cochlea. In fact, our estimates of the power law parameters of a (the central conduction 
591 
time), d (the frequency dependence) and b (the latency corresponding to 1 kHz and 65 dB SPL) for wave V 
592 
fell within the corresponding ranges that were previously reported for tone pips and derived-bands at 65 dB 
593 
ppeSPL (Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009). Interestingly, the frequency-
594 
specific responses across frequency band were similar in amplitude, or possibly slightly smaller for the 
595 
lower frequency bands (Figures 6, 8, 9), even though the relative energy of each band decreased with 
596 
increasing frequency, resulting in a ~30 dB difference between the lowest and highest frequency bands 
597 
(Figure 14). A greater response elicited by higher frequency bands is consistent with the relatively greater 
598 
contribution of neurons with higher characteristic frequencies to ABRs (Abdala and Folsom, 1995), as well 
599 
as the need for higher levels to elicit low frequency responses to tone pips that are close to threshold 
600 
(Gorga et al., 2006, 1993; Hyde, 2008; Stapells and Oates, 1997). Also, canonical waveforms were derived 
601 
in the higher frequency bands of diotically presented speech, with waves I and III identifiable in most 
602 
subjects. Multiband peaky speech will not replace the current frequency specific ABR, but there are 
603 
situations where it may be advantageous to use speech over tone pips. Measuring waves I, III, and V of 
604 
high frequency responses in the context of all the dynamics of speech may have applications to studying 
605 
effects of cochlear synaptopathy on speech comprehension (Bharadwaj et al., 2014; Liberman et al., 
606 
2016). Another exciting potential application is the evaluation of supra-threshold hearing across frequency 
607 
in toddlers and individuals who do not provide reliable behavioral responses, as they may be more 
608 
responsive to sitting for longer periods of time while listening to a narrated story than to a series of tone 
609 
pips. Giving a squirmy toddler an iPad and presenting a story for 30 minutes could allow responses to 
610 
multiband peaky speech that confirm audibility at normal speech levels. Such a screening or metric of 
611 
audibility is a useful piece of knowledge in pediatric clinical management and is also being investigated for 
612 
the frequency following response (Easwar et al., 2020, 2015), which provides different information than the 
613 
canonical responses. An extension of this assessment would be to evaluate neural speech processing in 
614 
the context of hearing loss, as well as rehabilitation strategies such as hearing aids and cochlear implants. 
615 
Auditory prostheses have algorithms specifically tuned for the spectrotemporal dynamics of speech that 
616 
behave very differently in response to standard diagnostic stimuli such as trains of clicks or tone pips. 
617 
Peaky speech responses could allow us to assess how the auditory system is encoding the amplified 
618 
speech and validate audibility of the hearing aid fittings before the infant or toddler is old enough to provide 
619 
reliable speech perception testing. Therefore, the ability of peaky speech to yield both canonical waveforms 
620 
and frequency-specific responses makes this paradigm a flexible method that assesses speech encoding 
621 
in new ways. 
622 
 
623 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
24 
Practical considerations for using peaky speech and deconvolution 
624 
Filtering 
625 
Having established that peaky speech is a flexible stimulus for investigating different aspects of 
626 
speech processing, there are several practical considerations for using the peaky speech paradigm. First, 
627 
filtering should be performed carefully. As recommended in Maddox and Lee (2018), causal filters – which 
628 
have impulse responses with non-zero values at positive lags only – should be used to ensure cortical 
629 
activity at later peak latencies does not spuriously influence earlier peaks corresponding to subcortical 
630 
origins. Applying less aggressive, low-order filters (i.e., broadband with shallow roll-offs) will help reduce 
631 
the effects of causal filtering on delaying response latency. The choice of high-pass cutoff will also affect 
632 
the response amplitude and morphology. After evaluating several orders and cutoffs to the high-pass 
633 
filters, we determined that early waves of the peaky broadband ABRs were best visualized with a 150 Hz 
634 
cutoff, whereas a lower cutoff frequency of 30 Hz was necessary to view the ABR and MLR of the 
635 
broadband responses. When evaluating specific contributions to the earliest waves, we recommend at 
636 
least a first order 150 Hz high-pass filter or a more aggressive second order 200 Hz high-pass filter to deal 
637 
with artifacts arising from nonlinearities that are not taken into account by the pulse train regressor or any 
638 
potential influences by responses from later sources (Figure 3). For multiband responses, the 150 Hz high-
639 
pass filter significantly reduced the response but also decreased the low-frequency noise in the pre-
640 
stimulus interval. For the 4-band multiband peaky speech the 150 Hz and 30 Hz filters provided similar 
641 
acquisition times for 0 dB SNR, but better SNRs were obtained quicker with 150 Hz filtering for the 10-band 
642 
multiband peaky speech. 
643 
Choice of narrator (stimulation rate) 
644 
Second, the choice of narrator impacts the responses to both broadband and multiband peaky 
645 
speech. Although overall morphology was similar, the male-narrated responses were larger, contained 
646 
more clearly identifiable component waves in a greater proportion of subjects, and achieved a 0 dB SNR at 
647 
least 2.1 to over 10 times faster than those evoked by a female narrator. These differences likely stemmed 
648 
from the ~77 Hz difference in average pitch, as higher stimulation rates evoke smaller responses due to 
649 
adaptation and refractoriness (e.g., Burkard et al., 1990; Burkard and Hecox, 1983; Chiappa et al., 1979; 
650 
Don et al., 1977; Jiang et al., 2009). Indeed, a 50 Hz change in fundamental frequency yields a 24% 
651 
reduction in the modelled auditory brainstem response that was derived as the complex cross-correlation 
652 
with the fundamental frequency (Saiz-Alia and Reichenbach, 2020). The narrator differences exhibited in 
653 
the present study may be larger than those in other studies with continuous speech (Forte et al., 2017; 
654 
Maddox and Lee, 2018; Saiz-Alía et al., 2019) as a result of the different regressors. These response 
655 
differences do not preclude using narrators with higher fundamental frequencies in future studies, but the 
656 
time required for usable responses from each narrator must be considered when planning experiments, 
657 
and caution taken when interpreting comparisons between conditions with differing narrators. The 
658 
strongest results will come from comparing responses to the same narrator (or even the same speech 
659 
recordings) under different experimental conditions. 
660 
SNR and recording time for multiple responses 
661 
Third, the necessary recording time depends on the chosen SNR threshold, experimental demands, 
662 
and stimulus. We chose a threshold SNR of 0 dB based on when waveforms became clearly identifiable, 
663 
but of course a different threshold would change our recording time estimates (though, notably, not the 
664 
ratios between them). With this SNR threshold, acquisition times were quick enough for broadband peaky 
665 
responses to allow multiple conditions in a reasonable recording session. With male-narrated broadband 
666 
peaky speech, all subjects achieved 0 dB SNR ABRs in < 5 minutes and MLRs in < 20 minutes, thereby 
667 
affording between 3 and 12 conditions in an hour recording session. These recording times are 
668 
comparable, if not faster, than the 8 minutes for the broad response to unaltered speech, 6–12 minutes for 
669 
the chirp-embedded speech (Backer et al., 2019), ~10 minutes for the broad complex-cross correlation 
670 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
25 
response to the fundamental waveform (Forte et al., 2017), and 33 minutes for the broad response to high-
671 
passed continuous speech (Maddox and Lee, 2018). However, using a narrator with a higher fundamental 
672 
frequency could increase testing time by 2- to over 10-fold. In this experiment, at most 2 conditions per 
673 
hour could be tested with the female-narrated broadband peaky speech. Unlike broadband peaky speech, 
674 
the testing times required for all frequency-specific responses to reach 0 dB SNR were significantly longer, 
675 
making only 1 condition feasible within a recording session. At least 30 minutes was necessary for the 
676 
multiband peaky speech, but based on extrapolated testing times, about 1 hour is required for 90% of 
677 
subjects and 2 hours for 75% of subjects to achieve this threshold for all 4 bands of diotic speech and all 
678 
10 bands of dichotic speech respectively. The longer testing times are important to consider when planning 
679 
studies using multiband peaky speech with several frequency bands.  
680 
Number of frequency bands 
681 
Fourth, as mentioned above, the number of frequency bands incorporated into multiband peaky 
682 
speech decreases SNR and increases testing time. Although it is possible to simultaneously record up to 
683 
10 frequency-specific responses, the significant time required to obtain decent SNRs reduces the feasibility 
684 
of testing multiple conditions or having recording sessions lasting less than 1–2 hours. However, pursuing 
685 
shorter testing times with multiband peaky speech is possible. Depending on the experimental question, 
686 
different multiband options could be considered. For male-narrated speech, the 2–4 and 4–8 kHz 
687 
responses had good SNRs and exhibited waves I, III, and V within 9 minutes for 90% of subjects. 
688 
Therefore, if researchers were more interested in comparing responses in these higher frequency bands, 
689 
they could stop recording once these bands reach threshold but before the lower frequency bands reach 
690 
criterion (i.e., within 9 minutes). Alternatively, the lower frequencies could be combined into a single 
691 
broader band in order to reduce the total number of bands, or the intensity could be increased to evoke 
692 
responses with larger amplitudes. Therefore, different band and parameter considerations could reduce 
693 
testing time and improve the feasibility, and thus utility, of multiband peaky speech. 
694 
Flexible analysis windows for deriving responses from auditory nerve to cortex 
695 
Fifth, and finally, a major advantage of deconvolution analysis is that the analysis window for the 
696 
response can be extended arbitrarily in either direction to include a broader range of latencies (Maddox 
697 
and Lee, 2018). Extending the pre-stimulus window leftward provides a better estimate of the SNR, and 
698 
extending the window rightward allows parts of the response that come after the ABR and MLR to be 
699 
analyzed as well, which are driven by the cortex. These later responses can be evaluated in response to 
700 
broadband peaky speech, but as shown in Figures 8 and 9, only ABR and early MLR waves are present in 
701 
the frequency-specific responses. The same broadband peaky speech data from Figure 5 are high-pass 
702 
filtered at 1 Hz and displayed with an extended time window in Figure 12, which shows component waves 
703 
of the ABR, MLR and late latency responses (LLR). Thus, this method allows us to simultaneously 
704 
investigate speech processing ranging from the earliest level of the auditory nerve all the way through the 
705 
cortex without requiring extra recording time. Usually the LLR is larger than the ABR/MLR, but our subjects 
706 
were encouraged to relax and rest, yielding a passive LLR response. Awake and attentive subjects may 
707 
improve the LLR; however, other studies that present continuous speech to attentive subjects also report 
708 
smaller and different LLR (Backer et al., 2019; Maddox and Lee, 2018), possibly from cortical adaptation to 
709 
a continuous stimulus. Here we used a simple 2-channel montage that is optimized for recording ABRs, but 
710 
a full multi-channel montage could also be used to more fully explore the interactions between subcortical 
711 
and cortical processing of naturalistic speech. The potential for new knowledge about how the brain 
712 
processes naturalistic and engaging stimuli cannot be undersold. 
713 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
26 
 
714 
 
715 
Peaky speech is a tool that opens up new lines of query 
716 
The peaky speech paradigm is a viable method for recording broadband and frequency-specific 
717 
responses from distinct subcortical structures using an engaging, continuous speech stimulus. The 
718 
customizability and flexibility of peaky speech facilitates new lines of query, both in neuroscientific and 
719 
clinical domains. Speech often occurs within a mixture of sounds, such as other speech sources, 
720 
background noise, or music. Furthermore, visual cues from a talker’s face are often available to aid speech 
721 
understanding, particularly in environments with low SNR (e.g., Bernstein and Grant, 2009; Grant et al., 
722 
2007). Peaky speech facilitates investigation into the complex subcortical encoding and processing that 
723 
underpins successful listening in these scenarios using naturalistic, engaging tasks. Indeed, previous 
724 
methods have been quite successful in elucidating cortical processing of speech under these conditions 
725 
(O’Sullivan et al., 2019; Teoh and Lalor, 2019). Whereas these cortical studies could use regressors that 
726 
are not acoustically-based – such as semantics or surprisal – the fast responses of subcortical structures 
727 
necessitate a regressor that can allow timing of components to separate subcortical from cortical origins. 
728 
The similarity of the peaky speech response to the click response is an advantage because it is the only 
729 
way to understand what is occurring in separate subcortical regions during the dynamic spectrotemporal 
730 
context of speech. As with other work in this area, how informative the response is about speech 
731 
processing will depend on how it is deployed experimentally. Experiments that measure the response to 
732 
the same stimuli in different cognitive states (unattended/attended, understood/not understood) will 
733 
illuminate the relationship between those states and subcortical encoding. Such an approach was recently 
734 
used to show how the brainstem complex cross-correlation to a fundamental waveform can change based 
735 
on top-down attention (Forte et al., 2017). Finally, the ability to customize peaky speech for measuring 
736 
frequency-specific responses provides potential applications to clinical research in the context of facilitating 
737 
assessment of supra-threshold hearing function and changes to how speech may be encoded following 
738 
intervention strategies and technologies while using a speech stimulus that algorithms in hearing aids and 
739 
cochlear implants are designed to process. 
740 
 
741 
 
Figure 12. The range of lags can be extended to allow early, middle and late latency responses to be analyzed from the same 
recording to broadband peaky speech. Average waveforms across subjects (areas show ± 1 SEM) are shown for responses 
measured to 32 minutes of broadband peaky speech narrated by a male (dark blue) and female (light blue). Responses were 
high-pass filtered at 1 Hz using a first order Butterworth filter, but different filter parameters can be used to focus on each stage 
of processing. Canonical waves of the ABR, MLR and LLR are labeled for the male-narrated speech. Due to adaptation, 
amplitudes of the late potentials are smaller than typically seen with other stimuli that are shorter in duration with longer inter-
stimulus intervals than our continuous speech. Waves I and III become more clearly visible by applying a 150 Hz high-pass 
cutoff. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
27 
METHODS 
742 
Participants 
743 
Data were collected over 3 experiments that were conducted under a protocol approved by the 
744 
University of Rochester Research Subjects Review Board (#1227). All subjects gave informed consent 
745 
before the experiment began and were compensated for their time. In each of experiments 1 and 2, there 
746 
were equipment problems during testing for one subject, rendering data unusable in the analyses. 
747 
Therefore, there were a total of 22, 11, and 11 subjects included in experiments 1, 2, and 3 respectively. 
748 
Four subjects completed both experiments 1 and 2, and 2 subjects completed both experiments 2 and 3. 
749 
The 38 unique subjects (25 females, 66%) were aged 18–32 years with a mean ± SD age of 23.0 ± 3.6 
750 
years. Audiometric screening confirmed subjects had normal hearing in both ears, defined as thresholds ≤ 
751 
20 dB HL from 250 to 8000 Hz. All subjects identified English as their primary language. 
752 
 
753 
Stimulus presentation and EEG measurement 
754 
In each experiment, subjects listened to 128 minutes of continuous speech stimuli while reclined in 
755 
a darkened sound booth. They were not required to attend to the speech and were encouraged to relax 
756 
and to sleep. Speech was presented at an average level of 65 dB SPL over ER-2 insert earphones 
757 
(Etymotic Research, Elk Grove, IL) plugged into an RME Babyface Pro digital soundcard (RME, 
758 
Haimhausen, Germany) via an HB7 headphone amplifier (Tucker Davis Technologies, Alachua, FL). 
759 
Stimulus presentation was controlled by a custom python (Python Programming Language, 
760 
RRID:SCR_008394) script using publicly available software (Expyfun, RRID:SCR_019285; available at 
761 
https://github.com/LABSN/expyfun; Larson et al., 2014). We interleaved conditions in order to prevent slow 
762 
impedance drifts or transient periods of higher EEG noise from unevenly affecting one condition over the 
763 
others. Physical measures to reduce stimulus artifact included: 1) hanging earphones from the ceiling so 
764 
that they were as far away from the EEG cap as possible; and 2) sending an inverted signal to a dummy 
765 
earphone (blocked tube) attached in the same physical orientation to the stimulus presentation earphones 
766 
in order to cancel electromagnetic fields away from transducers. The soundcard also produced a digital 
767 
signal at the start of each epoch, which was converted to trigger pulses through a custom trigger box 
768 
(modified from a design by the National Acoustic Laboratories, Sydney, NSW, Australia) and sent to the 
769 
EEG system so that audio and EEG data could be synchronized with sub-millisecond precision. 
770 
EEG was recorded using BrainVision’s PyCorder software (RRID:SCR_019286). Ag/AgCl 
771 
electrodes were placed at the high forehead (FCz, active non-inverting), left and right earlobes (A1, A2, 
772 
inverting references), and the frontal pole (Fpz, ground). These were plugged into an EP-Preamp system 
773 
specifically for recording ABRs, connected to an ActiCHamp recording system, both manufactured by 
774 
BrainVision. Data were sampled at 10,000 Hz and high-pass filtered at 0.1 Hz. Offline, raw data were high-
775 
pass filtered at 1 Hz using a first-order causal Butterworth filter to remove slow drift in the signal, and then 
776 
notch filtered with 5 Hz wide second-order infinite impulse response (IIR) notch filters to remove 60 Hz and 
777 
its first 3 odd harmonics (180, 300, 420 Hz). To optimize parameters for viewing the ABR and MLR 
778 
components of peaky speech responses, we evaluated several orders and high-pass cutoffs to the filters. 
779 
Early waves of the broadband peaky ABRs were best visualized with a 150 Hz cutoff, whereas a lower 
780 
cutoff frequency of 30 Hz was necessary to view the ABR and MLR of the broadband responses. 
781 
Conservative filtering with a first order filter was sufficient with these cutoff frequencies. 
782 
 
783 
Speech stimuli and conditions 
784 
Speech stimuli were taken from two audiobooks. The first was The Alchemyst (Scott, 2007), read 
785 
by a male narrator and used in all 3 experiments. The second was A Wrinkle in Time (L’Engle, 2012), read 
786 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
28 
by a female narrator and used in experiments 2 and 3. These stimuli were used in Maddox and Lee (2018), 
787 
but in that study a gentle high-pass filter was applied which was not done for this study. Briefly, the 
788 
audiobooks were resampled to 44,100 Hz and then silent pauses were truncated to 0.5 s. Speech was 
789 
segmented into 64 s epochs with 1 s raised cosine fade-in and fade-out. Because conditions were 
790 
interleaved, the last 4 s of a segment were repeated in the next segment so that subjects could pick up 
791 
where they left off if they were listening. 
792 
In experiment 1 subjects listened to 3 conditions of male speech (42.7 min each): unaltered speech, 
793 
re-synthesized broadband peaky speech, and re-synthesized multiband peaky speech (see below for a 
794 
description of re-synthesized speech). In experiment 2 subjects listened to 4 conditions of re-synthesized 
795 
peaky speech (32 minutes each): male and female narrators of both broadband and multiband peaky 
796 
speech. For these first 2 experiments, speech was presented diotically (same speech to both ears). In 
797 
experiment 3 subjects listened to both male and female dichotic (slightly different stereo speech of the 
798 
same narrator in each ear) multiband peaky speech designed for audiological applications (64 min of each 
799 
narrator). The same 64 s of speech was presented simultaneously to each ear, but the stimuli were dichotic 
800 
due to how the re-synthesized multiband speech was created (see below). 
801 
 
802 
Stimulus design 
803 
The brainstem responds best to impulse-like stimuli, so we re-synthesized the speech segments 
804 
from the audiobooks (termed “unaltered”) to create 3 types of “peaky” speech, with the objectives of 1) 
805 
evoking additional waves of the ABR reflecting other neural generators, and 2) measuring responses to 
806 
different frequency regions of the speech. The process is described in detail below, but is best read in 
807 
tandem with the code that is publicly available (https://github.com/maddoxlab). Figure 13 compares the 
808 
unaltered speech and re-synthesized broadband and multiband peaky speech. Comparing the pressure 
809 
waveforms shows that the peaky speech is as click-like as possible, but comparing the spectrograms (how 
810 
sound varies in amplitude at every frequency and time point) shows that the overall spectrotemporal 
811 
content that defines speech is basically unchanged by the re-synthesis. Audio files 1–6 provide examples 
812 
of each stimulus type for both narrators, which demonstrate the barely perceptible difference between 
813 
unaltered and peaky speech. 
814 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
29 
 
815 
Broadband peaky speech 
816 
Voiced speech comprises rapid openings and closings of the vocal folds which are then filtered by 
817 
the mouth and vocal tract to create different vowel and consonant sounds. The first processing step in 
818 
creating peaky speech was to use speech processing software (PRAAT, RRID:SCR_016564; Boersma 
819 
and Weenink, 2018) to extract the times of these glottal pulses. Sections of speech where glottal pulses 
820 
were within 17 ms of each other were considered voiced (vowels and voiced consonants like /z/). 17 ms is 
821 
the longest inter-pulse interval one would expect in natural speech because it is the inverse of 60 Hz, the 
822 
lowest pitch at which someone with a deep voice would likely speak. A longer gap in pulse times was 
823 
considered a break between voiced sections. These segments were identified in a “mixer” function of time, 
824 
with indices of 1 indicating unvoiced and 0 indicating voiced segments (and would later be responsible for 
825 
time-dependent blending of re-synthesized and natural speech, hence its name). Transitions of the binary 
826 
mixer function were smoothed using a raised cosine envelope spanning the time between the first and 
827 
second pulses, as well as the last two pulses of each voiced segment. During voiced segments, the glottal 
828 
pulses set the fundamental frequency of speech (i.e., pitch), which were allowed to vary from a minimum to 
829 
maximum of 60–350 Hz for the male narrator and 90–500 Hz for the female narrator. For the male and 
830 
female narrators, these pulses gave a mean ± SD fundamental frequency (i.e., pulse rate) in voiced 
831 
segments of 115.1 ± 6.7 Hz and 198.1 ± 20 Hz respectively, and a mean ± SD pulses per second over the 
832 
entire 64 s, inclusive of unvoiced periods and silences, of 69.1 ± 5.7 Hz and 110.8 ± 11.4 respectively. 
833 
These pulse times were smoothed using 10 iterations of replacing pulse time 𝑝𝑖 with the mean of pulse 
834 
 
Figure 13. Unaltered speech waveform (top left) and spectrogram (top right) compared to re-synthesized broadband peaky 
speech (middle left and right) and multiband peaky speech (bottom left and right). Comparing waveforms shows that the peaky 
speech is as “click-like” as possible, while comparing the spectrograms shows that the overall spectrotemporal content that 
defines speech is basically unchanged by the re-synthesis. A naïve listener is unlikely to notice that any modification has been 
performed, and subjective listening confirms the similarity. Yellow/lighter colors represent larger amplitudes than purple/darker 
colors in the spectrogram. Audio files 1–6 provide examples of each stimulus type for both narrators. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
30 
times 𝑝𝑖−1 to 𝑝𝑖+1 if the log2  absolute difference in the time between 𝑝𝑖 and 𝑝𝑖−1  and 𝑝𝑖+1  was less than 
835 
log2(1.6). 
836 
The fundamental frequency of voiced speech is dynamic, but the signal always consists of a set of 
837 
integer-related frequencies (harmonics) with different amplitudes and phases. To create the waveform 
838 
component at the fundamental frequency, 𝑓0(𝑡), we first created a phase function, 𝜑(𝑡), which increased 
839 
smoothly by 2𝜋 between glottal pulses within the voiced sections as a result of cubic interpolation. We then 
840 
computed the spectrogram of the unaltered speech waveform – which is a way of analyzing sound that 
841 
shows its amplitude at every time and frequency (Figure 13, top-right) – which we called 𝐴[𝑡, 𝑓0(𝑡)]. We 
842 
then created the fundamental component of the peaky speech waveform as: 
843 
ℎ0(𝑡) = 𝐴[𝑡, 𝑓0(𝑡)]cos[𝜑(𝑡)]. 
844 
This waveform has an amplitude that changes according to the spectrogram but always peaks at the time 
845 
of the glottal pulses. 
846 
Next the harmonics of the speech were synthesized. The 𝑘th harmonic of speech is at a frequency 
847 
of (𝑘+ 1)𝑓0 so we synthesized each harmonic waveform as: 
848 
ℎ𝑘(𝑡) = 𝐴[𝑡, (𝑘+ 1)𝑓0(𝑡)] cos[(𝑘+ 1)𝜑(𝑡)]. 
849 
Each of these harmonic waveforms has multiple peaks per period of the fundamental, but every harmonic 
850 
also has a peak at exactly the time of the glottal pulse. Because of these coincident peaks, when the 
851 
harmonics are summed to create the re-synthesized voiced speech, there is always a large peak at the 
852 
time of the glottal pulse. In other words, the phases of all the harmonics align at each glottal pulse, making 
853 
the pressure waveform of the speech appear “peaky” (left-middle panel of Figure 13). 
854 
The resultant re-synthesized speech contained only the voiced segments of speech and was 
855 
missing unvoiced sounds like /s/ and /k/. Thus the last step was to mix the re-synthesized voiced segments 
856 
with the original unvoiced parts. This was done by cross-fading back and forth between the unaltered 
857 
speech and re-synthesized speech during the unvoiced and voiced segments respectively, using the binary 
858 
mixer function created when determining where the voiced segments occurred. We also filtered the peaky 
859 
speech to an upper limit of 8 kHz, and used the unaltered speech above 8 kHz, to improve the quality of 
860 
voiced consonants such as /z/. Filter properties for the broadband peaky speech are further described 
861 
below in the “Band filters” subsection. 
862 
 
863 
Multiband peaky speech 
864 
The same principles to generate broadband peaky speech were applied to create stimuli designed 
865 
to investigate the brainstem’s response to different frequency bands that comprise speech. This makes use 
866 
of the fact that over time, speech signals with slightly different 𝑓0 are independent, or have (nearly) zero 
867 
cross-correlation, at the lags for the ABR. To make each frequency band of interest independent, we 
868 
shifted the fundamental frequency and created a fundamental waveform and its harmonics as: 
869 
ℎ𝑘(𝑡) = 𝐴[𝑡, (𝑘+ 1)𝑓0(𝑡)] cos[(𝑘+ 1)𝜑(𝑡)],  
870 
where 
871 
𝜑(𝑡) = 2𝜋∫(𝑓0(𝜏) + 𝑓∆)𝑑𝜏
𝑡
0
, 
872 
and where 𝑓∆ is the small shift in fundamental frequency.  
873 
In these studies, we increased fundamentals for each frequency band by the square root of each 
874 
successive prime number and subtracting one, resulting in a few tenths of a hertz difference between 
875 
bands. The first, lowest frequency band contained the un-shifted 𝑓0. Responses to this lowest, un-shifted 
876 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
31 
frequency band showed some differences from the common component for latencies > 30 ms that were not 
877 
present in the other, higher frequency bands (Figure 6, 0–1 kHz band), suggesting some low-frequency 
878 
privilege/bias in this response. Therefore, we suggest that following studies create independent frequency 
879 
bands by synthesizing a new fundamental for each band. The static shifts described above could be used, 
880 
but we suggest an alternative method that introduces random dynamic frequency shifts of up to ±1 Hz over 
881 
the duration of the stimulus. From this random frequency shift we can compute a dynamic random phase 
882 
shift, to which we also add a random starting phase, 𝜃∆, which is drawn from a uniform distribution between 
883 
0 and 2𝜋. The phase function from the above set of formulae would be replaced with this random dynamic 
884 
phase function: 
885 
𝜑(𝑡) = 2𝜋∫[𝑓0(𝜏) + 𝑓∆(𝜏)]𝑑𝜏+ 𝜃∆
𝑡
0
 
886 
Validation data from one subject is provided in Figure 6-figure supplement 1. Responses from all four 
887 
bands show more consistent resemblance to the common component, indicating that this method is 
888 
effective at reducing stimulus-related bias. However, low-frequency dependent differences remained, 
889 
suggesting there is also unique neural-based low-frequency activity to the speech-evoked responses. 
890 
This re-synthesized speech was then band-pass filtered to the frequency band of interest (e.g. from 
891 
0–1 kHz or 2–4 kHz). This process was repeated for each independent frequency band, then the bands 
892 
were mixed together and then these re-synthesized voiced parts were mixed with the original unaltered 
893 
voiceless speech. This peaky speech comprised octave bands with center frequencies of: 707, 1414, 2929, 
894 
5656 Hz for experiments 1 and 2, and of 500, 1000, 2000, 4000, 8000 Hz for experiment 3. Note that for 
895 
the lowest band, the actual center frequency was slightly lower because the filters were set to pass all 
896 
frequencies below the upper cutoff. Filter properties for these two types of multiband speech are shown in 
897 
the middle and right panels of Figure 16 and further described below in the “Band filters” subsection. For 
898 
the dichotic multiband peaky speech, we created 10 fundamental waveforms – 2 in each of the five filter 
899 
bands for the two different ears, making the output audio file stereo (or dichotic). We also filtered this 
900 
dichotic multiband peaky speech to an upper limit of 11.36 kHz to allow for the highest band to have a 
901 
center frequency of 8 kHz and octave width. The relative mean-squared magnitude in decibels for 
902 
components of the multiband peaky speech (4 filter bands) and dichotic (audiological) multiband peaky 
903 
speech (5 filter bands) are shown in Figure 14. 
904 
 
905 
 
Figure 14. Relative mean-squared magnitude in decibels of multiband peaky speech with 4 filter bands (left) and 5 filter bands 
(right) for male-(blue) and female-(orange) narrated speech. The full audio comprises unvoiced and re-synthesized voiced 
sections, which was presented to the subjects during the experiments. The other bands reflect the relative magnitude of the 
voiced sections (voiced only), and each filtered frequency band. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
32 
For peaky speech, the re-synthesized speech waveform was presented during the experiment but 
906 
the pulse trains were used as the input stimulus for calculating the response (i.e., the regressor, see 
907 
Response derivation section below). These pulse trains all began and ended together in conjunction with 
908 
the onset and offset of voiced sections of the speech. To verify which frequency ranges of the multiband 
909 
pulse trains were independent across frequency bands, and would thus yield truly band-specific responses, 
910 
we conducted a spectral coherence analyses on the pulse trains. All 60 unique 64 s sections of each male- 
911 
and female-narrated multiband peaky speech used in the three experiments were sliced into 1 s segments 
912 
for a total of 3,840 slices. Phase coherence across frequency was then computed across these slices for 
913 
each combination of pulse trains according to the formula: 
914 
𝐶𝑥𝑦= 
| 𝐸[ℱ{𝑥𝑖}∗ ℱ{𝑦𝑖}] |
√𝐸[ℱ{𝑥𝑖}∗ ℱ{𝑥𝑖}] 𝐸[ℱ{𝑦𝑖}∗ ℱ{𝑦𝑖}] 
 
915 
where 𝐶𝑥𝑦 denotes coherence between bands 𝑥 and 𝑦, 𝐸[ ] the average across slices, ℱ the fast Fourier 
916 
transform, * complex conjugation, 𝑥𝑖 the pulse train for slice 𝑖 in band 𝑥, and 𝑦𝑖 the pulse train for slice 𝑖 in 
917 
band 𝑦. 
918 
Spectral coherence for each narrator is shown in Figure 15. For the 4-band multiband peaky 
919 
speech used in experiments 1 and 2 there were 6 pulse train comparisons. For the audiological multiband 
920 
peaky speech used in experiment 3, there were 5 bands for each of 2 ears, resulting in 10 pulse trains and 
921 
45 comparisons. All 45 comparisons are shown in Figure 15A for the stimuli created with static frequency 
922 
shifts, and the 6 comparisons for the stimuli created with random dynamic frequency shifts (used in the 
923 
pilot experiment) are shown in Figure 15B. Pulse trains were coherent (> 0.1) up to a maximum of 71 and 
924 
126 Hz for male- and female-narrated speech respectively, which roughly correspond to the mean ± SD 
925 
pulse rates (calculated as total pulses / 64 s) of 69.1 ± 5.7 Hz and 110.8 ± 11.4 respectively. This means 
926 
that above ~130 Hz the stimuli were no longer coherent and evoked frequency-specific responses. 
927 
Importantly, responses would be to correlated stimuli, i.e., not frequency-specific, at frequencies below this 
928 
cutoff and would result in a low-frequency response component that is present in (or common to) all band 
929 
responses. There were two separated collections of curves for the stimuli with static frequency shifts, which 
930 
stemmed from the shifted frequency bands having different coherence with each other than the lowest 
931 
unshifted band. This stimulus-related bias in the lowest frequency band was reduced by using dynamic 
932 
random frequency shifts (used in the pilot experiment), as indicated by the similar spectral coherence 
933 
curves shown in Figure 15B.  
934 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
33 
 
935 
To identify the effect of the low-frequency stimulus coherence in the responses, we computed the 
936 
common component across pulse trains by creating an averaged response to 6 additional “fake” pulses 
937 
trains that were created during stimulus design but were not used during creation of the multiband peaky 
938 
speech wav files. The common component was assessed for both “fake” pulse trains taken from shifts 
939 
lower than the original fundamental frequency and those taken from shifts higher than the highest “true” re-
940 
synthesized fundamental frequency. For the dynamic frequency shift method (pilot experiment) an 
941 
additional 6 pulse trains were created. Figure 15-figure supplement 1 shows that the common component 
942 
was similar for “fake” pulse trains created using static or dynamic random frequency shifts. To assess 
943 
frequency-specific responses to multiband speech, we subtracted this common component from the band 
944 
responses. Alternatively, one could simply high-pass the stimuli at 150 Hz using a first-order causal 
945 
Butterworth filter (being mindful of edge artifacts). However, this high-pass filtering reduces response 
946 
amplitude and may affect response detection (see Results for more details).  
947 
We also verified the independence of the stimulus bands by treating the regressor pulse train as the 
948 
input to a system whose output was the rectified stimulus audio and performed deconvolution (see 
949 
Deconvolution and Response derivation section below). Further details are provided in Figure 15-figure 
950 
supplement 2. The responses showed that the non-zero responses only occurred when the correct pulse 
951 
train was paired with the correct audio. 
952 
 
953 
Band filters 
954 
Because the fundamental frequencies for each frequency band were designed to be independent 
955 
over time, the band filters for the speech were designed to cross over in frequency at half power. To make 
956 
the filter, the amplitude was set by taking the square root of the specified power at each frequency. Octave 
957 
band filters were constructed in the frequency domain by applying trapezoids – with center bandwidth and 
958 
roll-off widths of 0.5 octaves. For the first (lowest frequency) band, all frequencies below the high-pass 
959 
cutoff were set to 1, and likewise for all frequencies above the low-pass cutoff for the last (highest 
960 
frequency) band were set to 1 (Figure 16 top row). The impulse response of the filters was assessed by 
961 
shifting the inverse FFT (IFFT) of the bands so that time zero was in the center, and then applied a Nuttall 
962 
window, thereby truncating the impulse response to length of 5 ms (Fig 16 middle row). The actual 
963 
 
Figure 15. Spectral coherence of pulse trains for multiband peaky speech narrated by a male and female. Spectral coherence 
was computed across 1 s slices from 60 unique 64 s multiband peaky speech segments (3,840 total slices) for each 
combination of bands. Each light gray line represents the coherence for one band comparison. (A) There were 45 comparisons 
across the 10-band (audiological) speech used in experiment 3 (5 frequency bands x 2 ears). The lowest band was unshifted 
and the other 9 bands had static frequency shifts. (B) There were 6 comparisons across 4 pulse trains of the bands in the pilot 
experiment, which all had dynamic random frequency shifts. Pulse trains (i.e., the input stimuli, or regressors, for the 
deconvolution) were frequency-dependent (coherent) below 72 Hz for the male multiband speech and 126 Hz for the female 
multiband speech. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
34 
frequency response of the filter bands was assessed by taking the FFT of the impulse response and 
964 
plotting the magnitude (Figure 16 bottom row). 
965 
 
966 
 
967 
As mentioned above, broadband peaky speech was filtered to an upper limit of 8 kHz for diotic 
968 
peaky speech and 11.36 kHz for dichotic peaky speech. This band filter was constructed from the second 
969 
last octave band filter from the multiband filters (i.e., the 4–8 kHz band from the top-middle of Figure 16, 
970 
dark red line) by setting the amplitude of all frequencies less than the high-pass cutoff frequency to 1 
971 
(Figure 16 top-left panel, blue line). As mentioned above, unaltered (unvoiced) speech above 8 kHz (diotic) 
972 
or 11.36 kHz (dichotic) was mixed with the broadband and multiband peaky speech, which was 
973 
accomplished by applying the last (highest) octave band filter (8+ or 11.36+ kHz band, black line) to the 
974 
unaltered speech and mixing this band with the re-synthesized speech using the other bands. 
975 
 
976 
Alternating polarity 
977 
To limit stimulus artifact, we also alternated polarity between segments of speech. To identify 
978 
regions to flip polarity, the envelope of speech was extracted using a first-order causal Butterworth low-
979 
 
Figure 16. Octave band filters used to create re-synthesized broadband peaky speech (left, blue), diotic multiband peaky 
speech with 4 bands (middle, red), and dichotic multiband peaky speech using 5 bands with audiological center frequencies 
(right, red). The last band (2nd, 5th, 6th respectively, black line) was used to filter the high-frequencies of unaltered speech 
during mixing to improve the quality of voiced consonants. The designed frequency response using trapezoids (top) were 
converted into the time-domain using IFFT, shifted and Nuttall windowed to create impulse responses (middle), which were 
then used to assess the actual frequency response by converting into the frequency domain using FFT (bottom). 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
35 
pass filter with a cutoff frequency of 6 Hz applied to the absolute value of the waveform. Then flip indices 
980 
were identified where the envelope became less than 1 percent of the median envelope value, and then a 
981 
function that changed back and forth between 1 and −1 at each flip index was created. This function of 
982 
spikes was smoothed using another first-order causal Butterworth low-pass filter with a cutoff frequency of 
983 
10,000 Hz, which was then multiplied with the re-synthesized speech before saving to a wav file. 
984 
 
985 
Response derivation 
986 
Deconvolution 
987 
The peaky-speech ABR was derived by using deconvolution, as in previous work (Maddox and Lee, 
988 
2018), though the computation was performed in the frequency domain for efficiency. The speech was 
989 
considered the input to a linear system whose output was the recorded EEG signal, with the ABR 
990 
computed as the system’s impulse response. As in Maddox and Lee (2018), for the unaltered speech, we 
991 
used the half-wave rectified audio as the input waveform. Half-wave rectification was accomplished by 
992 
separately calculating the response to all positive and all negative values of the input waveform for each 
993 
epoch and then combining the responses together during averaging. For our new re-synthesized peaky 
994 
speech, the input waveform was the sequence of impulses that occurred at the glottal pulse times and 
995 
corresponded to the peaks in the waveform. Figure 17 shows a section of stimulus and the corresponding 
996 
input signal of glottal pulses used in the deconvolution.  
997 
 
998 
The half-wave rectified waveforms and glottal pulse sequences were corrected for the small clock 
999 
differences between the sound card and EEG system and then down-sampled to the EEG sampling 
1000 
frequency prior to deconvolution. To avoid temporal splatter due to standard downsampling, the pulse 
1001 
sequences were resampled by placing unit impulses at sample indices closest to each pulse time. 
1002 
Regularization was not necessary because the amplitude spectra of these regressors were sufficiently 
1003 
broadband. For efficiency, the time-domain response waveform, 𝑤, for a given 64 s epoch was calculated 
1004 
using frequency-domain division for the deconvolution, with the numerator the cross-spectral density 
1005 
(corresponding to the cross-correlation in the time domain) of the stimulus regressor and EEG response, 
1006 
and the denominator the power spectral density of the stimulus regressor (corresponding to its 
1007 
autocorrelation in the time domain). For a single epoch, that would be: 
1008 
 
Figure 17. Left: A segment of broadband peaky speech stimulus (top) and the corresponding glottal pulse train (bottom) used 
in calculating the broadband peaky speech response. Right: An example broadband peaky speech response from a single 
subject. The response shows ABR waves I, III, and V at ~3, 5, 7 ms respectively. It also shows later peaks corresponding to 
thalamic and cortical activity at ~17 and 27 ms respectively. 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
36 
𝑤= ℱ−1 {ℱ{𝑥}∗ ℱ{𝑦} 
ℱ{𝑥}∗ ℱ{𝑥}} 
1009 
where ℱ denotes the fast Fourier transform, ℱ−1 the inverse fast Fourier Transform, * complex conjugation, 
1010 
𝑥 the input stimulus regressor (half-wave rectified waveform or glottal pulse sequence), and 𝑦 the EEG 
1011 
data for each epoch. We used methods incorporated into the MNE-python package (RRID:SCR_005972; 
1012 
Gramfort et al., 2013). In practice, we made adaptations to this formula to improve the SNR with Bayesian-
1013 
like averaging (see below). For multiband peaky speech the same EEG was deconvolved with the pulse 
1014 
train of each band separately, and then with an additional 6 “fake” pulse trains to derive the common 
1015 
component across bands due to the pulse train coherence at low frequencies (shown in Figure 15). The 
1016 
averaged response across these 6 fake pulse trains, or common component, was then subtracted from the 
1017 
multiband responses to identify the frequency-specific band responses. 
1018 
 
1019 
Response averaging 
1020 
The quality of the ABR waveforms as a function of each type of stimulus was of interest, so we 
1021 
calculated the averaged response after each 64 s epoch. We followed a Bayesian-like process (Elberling 
1022 
and Wahlgreen, 1985) to account for variations in noise level across the recording time (such as slow drifts 
1023 
or movement artifacts) and to avoid rejecting data based on thresholds. Each epoch was weighted by its 
1024 
inverse variance, 1 𝜎𝑖
2
⁄
, to the sum of the inverse variances of all epochs. Thus, epoch weights, 𝑏𝑖, were 
1025 
calculated as follows: 
1026 
𝑏𝑖= 
1 𝜎𝑖
2
⁄
∑
1 𝜎𝑖
2
⁄
𝑛
𝑖=1
 
1027 
where 𝑖 is the epoch number and 𝑛 is the number of epochs collected. For efficiency, weighted averaging 
1028 
was completed during deconvolution. Because auto-correlation of the input stimulus (denominator of the 
1029 
frequency domain division) was similar across epochs it was averaged with equal weighting. Therefore, the 
1030 
numerator of the frequency domain division was summed across weighted epochs and the denominator 
1031 
averaged across epochs, according to the following formula:  
1032 
𝑤= ℱ−1 {
∑
𝑏𝑖ℱ{𝑥𝑖}∗ ℱ{𝑦𝑖} 
𝑛
𝑖=1
∑
1
𝑛ℱ{𝑥𝑖}∗ ℱ{𝑥𝑖}
𝑛
𝑖=1
} 
1033 
 
1034 
where 𝑤 is the average response waveform, 𝑖 is again the epoch number, 𝑛 is the number of epochs 
1035 
collected. 
1036 
Due to the circular nature of the discrete frequency domain deconvolution, the resulting response 
1037 
has an effective time interval of [0, 32] s at the beginning and [-32, 0) s at the end, so that concatenating 
1038 
the two – with the end first – yields the response from [-32, 32] s. Consequently, to avoid edge artifacts, all 
1039 
filtering was performed after the response was shifted to the middle of the 64 s time window. To remove 
1040 
high-frequency noise and some low-frequency noise, the average waveform was band-pass filtered 
1041 
between 30–2000 Hz using a first-order causal Butterworth filter. An example of this weighted average 
1042 
response to broadband peaky speech is shown in the right panel of Figure 17. This bandwidth of 30 to 
1043 
2000 Hz is sufficient to identify additional waves in the brainstem and middle latency responses (ABR and 
1044 
MLR respectively). To further identify earlier waves of the auditory brainstem responses (i.e., waves I and 
1045 
III), responses were high-pass filtered at 150 Hz using a first-order causal Butterworth filter. This filter was 
1046 
determined to provide the best morphology without compromising the response by comparing responses 
1047 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
37 
filtered with common high-pass cutoffs of 1, 30, 50, 100 and 150 Hz each combined with first, second and 
1048 
fourth order causal Butterworth filters.  
1049 
 
1050 
Response normalization 
1051 
An advantage of this method over our previous one (Maddox and Lee, 2018) is that because the 
1052 
regressor comprises unit impulses, the deconvolved response is given in meaningful units which are the 
1053 
same as the EEG recording, namely microvolts. With a continuous regressor, like the half-wave rectified 
1054 
speech waveform, this is not the case. Therefore, to compare responses to half-wave rectified speech 
1055 
versus glottal pulses, we calculated a normalization factor, 𝑔, based on data from all subjects: 
1056 
𝑔= 1 𝑛 ∑
𝜎𝑢,𝑖
𝑛
𝑖=1
⁄
1 𝑛 ∑
𝜎𝑝,𝑖
𝑛
𝑖=1
⁄
 
1057 
where 𝑛 is the number of subjects, 𝜎𝑢,𝑖 is the SD of subject  𝑖’s response to unaltered speech between 0–
1058 
20 ms, and 𝜎𝑝,𝑖 is the same for the broadband peaky speech. Each subject’s responses to unaltered 
1059 
speech were multiplied by this normalization factor to bring these responses within a comparable amplitude 
1060 
range as those to broadband peaky speech. Consequently, amplitudes were not compared between 
1061 
responses to unaltered and peaky speech. This was not our prime interest, rather we were interested in 
1062 
latency and presence of canonical component waves. In this study the normalization factor was 0.26, which 
1063 
cannot be applied to other studies because this number also depends on the scale when storing the digital 
1064 
audio. In our study, this unitless scale was based on a root-mean-square amplitude of 0.01. The same 
1065 
normalization factor was used when the half-wave rectified speech as used as the regressor with EEG 
1066 
collected in response to unaltered speech, broadband peaky speech and multiband peaky speech (Figure 
1067 
2, Figure 4, Figure 4-figure supplement 1). 
1068 
 
1069 
Response SNR calculation 
1070 
We were also interested in the recording time required to obtain robust responses to re-synthesized 
1071 
peaky speech. Therefore, we calculated the time it took for the ABR and MLR to reach a 0-dB SNR. The 
1072 
SNR of each waveform in dB, 𝑆𝑁𝑅𝑤, was estimated as: 
1073 
𝑆𝑁𝑅𝑤 = 10 log10 [𝜎𝑆+𝑁
2
−𝜎𝑁
2
σ𝑁
2
], 
1074 
where 𝜎𝑆+𝑁
2
 represents the variance (i.e., mean-subtracted energy) of the waveform between 0 and 15 ms 
1075 
or 30 ms for the ABR and MLR respectively (contains both component signals as well as noise, 𝑆+ 𝑁), and 
1076 
𝜎𝑁
2 represents the variance of the noise, 𝑁, estimated by averaging the variances of 15 ms (ABR) to 30 ms 
1077 
(MLR) segments of the pre-stimulus baseline between −480 and −20 ms. Then the SNR for 1 min of 
1078 
recording, 𝑆𝑁𝑅60, was computed from the 𝑆𝑁𝑅𝑤 as: 
1079 
𝑆𝑁𝑅60 = 𝑆𝑁𝑅𝑤+ 10 log10[60 𝑡𝑤
⁄
], 
1080 
where 𝑡𝑤 is the duration of the recording in seconds, as specified in the “Speech stimuli and conditions” 
1081 
subsection. For example, in experiment 3, the average waveform resulted from 64 min of recording, or a 𝑡𝑤  
1082 
of 3,840 s. The time to reach 0 dB SNR for each subject, 𝑡0𝑑𝐵 𝑆𝑁𝑅 , was estimated from this 𝑆𝑁𝑅60 by: 
1083 
𝑡0𝑑𝐵 𝑆𝑁𝑅= 60 × 10−𝑆𝑁𝑅60 10
⁄
. 
1084 
Cumulative density functions were used to show the proportion of subjects that reached an SNR ≥ 0 dB 
1085 
and to determine the necessary acquisition times that can be expected for each stimulus on a group level. 
1086 
 
1087 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
38 
Statistical Analyses 
1088 
Data were checked for normality using the Shapiro-Wilk test. Waveform morphology of responses 
1089 
to different narrators was compared using Pearson correlations of the responses between 0 and 15 ms for 
1090 
the ABR waveforms or 0 and 40 ms for both ABR and MLR waveforms. The Wilcoxon signed rank test was 
1091 
used to determine whether narrator differences (waveform correlations) were significantly different than the 
1092 
correlations of the same EEG split into even and odd epochs with equal numbers of epochs from each 
1093 
narrator. The intraclass correlation coefficient type 3 (absolute agreement) was used to verify good 
1094 
agreement in peak latencies chosen by an experienced audiologist and neuroscientist (MJP) at two 
1095 
different time points, 3 months apart. Independent t-tests with μ = 0 were conducted on the peak latency 
1096 
differences of ABR/MLR waves for unaltered and broadband peaky speech. For multiband peaky speech, 
1097 
the component wave peak latency changes across frequency band were assessed with power law 
1098 
regression (Harte et al., 2009; Neely et al., 1988; Rasetshwane et al., 2013; Strelcyk et al., 2009), 
1099 
conducted in the log-log domain with linear mixed effects regression using the lme4 (RRID:SCR_015654) 
1100 
and lmerTest (RRID:SCR_015656) packages in R (RRID:SCR_001905) and RStudio (RRID:SCR_000432) 
1101 
(Bates et al., 2015; Kuznetsova et al., 2017; R Core Team, 2020). The parameter 𝑎 of the power law 
1102 
regression was estimated by adding an assumed synaptic delay of 0.8 (Eggermont, 1979; Strelcyk et al., 
1103 
2009) to the I-V inter-wave delay from the subjects' responses to broadband peaky speech. For subjects 
1104 
who did not have an identifiable wave I in the broadband peaky response, the group mean I-V delay was 
1105 
used – this occurred for 1 of 22 subjects in experiment 1, and 2 of 11 subjects for responses to the female 
1106 
narrator in experiment 2. Because only multiband peaky speech was presented in experiment 3, the mean 
1107 
I-V intervals from experiment 2 were used for each subject for experiment 3. Random effects of subject and 
1108 
each frequency band term were included to account for individual variability that is not generalizable to the 
1109 
fixed effects. A power analysis was completed using the simR package (RRID:SCR_019287; Green and 
1110 
MacLeod, 2016), which uses a likelihood ratio test on 1000 Monte Carlo permutations of the response 
1111 
variables based on the fitted model. 
1112 
 
1113 
ACKNOWLEDGMENTS 
1114 
The authors wish to thank Sara Fiscella for assistance with recruitment. We would also like to thank the 
1115 
reviewers, whose suggestions significantly strengthened the manuscript. 
1116 
 
1117 
FUNDING 
1118 
This work was supported by National Institute for Deafness and Other Communication Disorders 
1119 
[R00DC014288] awarded to RKM. 
1120 
 
1121 
DATA AVAILABILITY 
1122 
Python code is available on the lab GitHub account (https://github.com/maddoxlab/peaky-speech). All EEG 
1123 
recordings are available in the EEG-BIDS format (Pernet et al., 2019) on Dryad 
1124 
(https://doi.org/10.5061/dryad.12jm63xwd). Stimulus files and python code necessary to derive the peaky 
1125 
speech responses are deposited to the same Dryad repository.  
1126 
 
1127 
REFERENCES 
1128 
Abdala C, Folsom RC. 1995. Frequency contribution to the click‐evoked auditory brain‐stem response in 
1129 
human adults and infants. J Acoust Soc Am 97:2394–2404. doi:10.1121/1.411961 
1130 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
39 
Backer KC, Kessler AS, Lawyer LA, Corina DP, Miller LM. 2019. A novel EEG paradigm to simultaneously 
1131 
and rapidly assess the functioning of auditory and visual pathways. J Neurophysiol 122:1312–1329. 
1132 
doi:10.1152/jn.00868.2018 
1133 
Bajo VM, King AJ. 2012. Cortical modulation of auditory processing in the midbrain. Front Neural Circuits 
1134 
6:114. doi:10.3389/fncir.2012.00114 
1135 
Bajo VM, Nodal FR, Moore DR, King AJ. 2010. The descending corticocollicular pathway mediates 
1136 
learning-induced auditory plasticity. Nat Neurosci 13:253–260. doi:10.1038/nn.2466 
1137 
Bates D, Mächler M, Bolker B, Walker S. 2015. Fitting Linear Mixed-Effects Models Using lme4. J Stat 
1138 
Softw 67:1–48. 
1139 
Bernstein JGW, Grant KW. 2009. Auditory and auditory-visual intelligibility of speech in fluctuating maskers 
1140 
for normal-hearing and hearing-impaired listeners. J Acoust Soc Am 125:3358–3372. 
1141 
doi:10.1121/1.3110132 
1142 
Bharadwaj HM, Verhulst S, Shaheen L, Liberman MC, Shinn-Cunningham BG. 2014. Cochlear neuropathy 
1143 
and the coding of supra-threshold sound. Front Syst Neurosci 8. doi:10.3389/fnsys.2014.00026 
1144 
Boersma P, Weenink D. 2018. Praat: doing phonetics by computer. 
1145 
Bramhall N, Beach EF, Epp B, Le Prell CG, Lopez-Poveda EA, Plack CJ, Schaette R, Verhulst S, Canlon 
1146 
B. 2019. The search for noise-induced cochlear synaptopathy in humans: Mission impossible? Hear 
1147 
Res 377:88–103. doi:10.1016/j.heares.2019.02.016 
1148 
Burkard R, Hecox K. 1983. The effect of broadband noise on the human brainstem auditory evoked 
1149 
response. I. Rate and intensity effects. J Acoust Soc Am 74:1204–1213. doi:10.1121/1.390024 
1150 
Burkard R, Shi Y, Hecox KE. 1990. A comparison of maximum length and Legendre sequences for the 
1151 
derivation of brain‐stem auditory‐evoked responses at rapid rates of stimulation. J Acoust Soc Am 
1152 
87:1656–1664. doi:10.1121/1.399413 
1153 
Carney LH, Li T, McDonough JM. 2015. Speech Coding in the Brain: Representation of Vowel Formants by 
1154 
Midbrain Neurons Tuned to Sound Fluctuations,,. eNeuro 2. doi:10.1523/ENEURO.0004-15.2015 
1155 
Chiappa KH, Gladstone KJ, Young RR. 1979. Brain Stem Auditory Evoked Responses: Studies of 
1156 
Waveform Variations in 50 Normal Human Subjects. Arch Neurol 36:81–87. 
1157 
doi:10.1001/archneur.1979.00500380051005 
1158 
Dau T, Wegner O, Mellert V, Kollmeier B. 2000. Auditory brainstem responses with optimized chirp signals 
1159 
compensating basilar-membrane dispersion. J Acoust Soc Am 107:1530–1540. 
1160 
doi:10.1121/1.428438 
1161 
Don M, Allen AR, Starr A. 1977. Effect of Click Rate on the Latency of Auditory Brain Stem Responses in 
1162 
Humans. Ann Otol Rhinol Laryngol 86:186–195. doi:10.1177/000348947708600209 
1163 
Easwar V, Purcell DW, Aiken SJ, Parsa V, Scollie SD. 2015. Evaluation of Speech-Evoked Envelope 
1164 
Following Responses as an Objective Aided Outcome Measure: Effect of Stimulus Level, 
1165 
Bandwidth, and Amplification in Adults With Hearing Loss. Ear Hear 36:635–652. 
1166 
doi:10.1097/AUD.0000000000000199 
1167 
Easwar V, Scollie S, Aiken S, Purcell D. 2020. Test-Retest Variability in the Characteristics of Envelope 
1168 
Following Responses Evoked by Speech Stimuli. Ear Hear 41:150–164. 
1169 
doi:10.1097/AUD.0000000000000739 
1170 
Eggermont JJ. 1979. Narrow-band AP latencies in normal and recruiting human ears. J Acoust Soc Am 
1171 
65:463–470. doi:10.1121/1.382345 
1172 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
40 
Elberling C, Don M. 2008. Auditory brainstem responses to a chirp stimulus designed from derived-band 
1173 
latencies in normal-hearing subjects. J Acoust Soc Am 124:3022–3037. doi:10.1121/1.2990709 
1174 
Elberling C, Wahlgreen O. 1985. Estimation of Auditory Brainstem Response, Abr, by Means of Bayesian 
1175 
Inference. Scand Audiol 14:89–96. doi:10.3109/01050398509045928 
1176 
Forte AE, Etard O, Reichenbach T. 2017. The human auditory brainstem response to running speech 
1177 
reveals a subcortical mechanism for selective attention. eLife 6:e27203. doi:10.7554/eLife.27203 
1178 
Geisler CD, Frishkopf LS, Rosenblith WA. 1958. Extracranial Responses to Acoustic Clicks in Man. 
1179 
Science 128:1210–1211. doi:10.1126/science.128.3333.1210 
1180 
Goldstein R, Rodman LB. 1967. Early components of averaged evoked responses to rapidly repeated 
1181 
auditory stimuli. J Speech Hear Res 10:697–705. doi:10.1044/jshr.1004.697 
1182 
Gorga MP, Johnson TA, Kaminski JR, Beauchaine KL, Garner CA, Neely ST. 2006. Using a Combination 
1183 
of Click- and Tone Burst–Evoked Auditory Brain Stem Response Measurements to Estimate Pure-
1184 
Tone Thresholds. Ear Hear 27:60–74. doi:10.1097/01.aud.0000194511.14740.9c 
1185 
Gorga MP, Kaminski JR, Beauchaine KA, Jesteadt W. 1988. Auditory brainstem responses to tone bursts 
1186 
in normally hearing subjects. J Speech Hear Res 31:87–97. doi:10.1044/jshr.3101.87 
1187 
Gorga MP, Kaminski JR, Beauchaine KL, Bergman BM. 1993. A Comparison of Auditory Brain Stem 
1188 
Response Thresholds and latencies Elicited by Air- and Bone-Conducted Stimuli. Ear Hear 14:85–
1189 
94. 
1190 
Gramfort A, Luessi M, Larson E, Engemann DA, Strohmeier D, Brodbeck C, Goj R, Jas M, Brooks T, 
1191 
Parkkonen L, Hämäläinen M. 2013. MEG and EEG data analysis with MNE-Python. Front Neurosci 
1192 
7. doi:10.3389/fnins.2013.00267 
1193 
Grant KW, Tufts JB, Greenberg S. 2007. Integration efficiency for speech perception within and across 
1194 
sensory modalities by normal-hearing and hearing-impaired individuals. J Acoust Soc Am 
1195 
121:1164–1176. doi:10.1121/1.2405859 
1196 
Green P, MacLeod CJ. 2016. SIMR: an R package for power analysis of generalized linear mixed models 
1197 
by simulation. Methods Ecol Evol 7:493–498. doi:10.1111/2041-210X.12504 
1198 
Grothe B, Pecka M. 2014. The natural history of sound localization in mammals--a story of neuronal 
1199 
inhibition. Front Neural Circuits 8:116. doi:10.3389/fncir.2014.00116 
1200 
Harte JM, Pigasse G, Dau T. 2009. Comparison of cochlear delay estimates using otoacoustic emissions 
1201 
and auditory brainstem responses. J Acoust Soc Am 126:1291–1301. doi:10.1121/1.3168508 
1202 
Hashimoto I. 1982. Auditory evoked potentials from the human midbrain: slow brain stem responses. 
1203 
Electroencephalogr Clin Neurophysiol 53:652–657. doi:10.1016/0013-4694(82)90141-9 
1204 
Hyde M. 2008. Ontario Infant Hearing Program Audiologic Assessment Protocol Version 3.1. 
1205 
Jiang ZD, Wu YY, Wilkinson AR. 2009. Age-related changes in BAER at different click rates from neonates 
1206 
to adults. Acta Paediatr Oslo Nor 1992 98:1284–1287. doi:10.1111/j.1651-2227.2009.01312.x 
1207 
Kileny P, Paccioretti D, Wilson AF. 1987. Effects of cortical lesions on middle-latency auditory evoked 
1208 
responses (MLR). Electroencephalogr Clin Neurophysiol 66:108–120. doi:10.1016/0013-
1209 
4694(87)90180-5 
1210 
Kuznetsova A, Brockhoff PB, Christensen RHB. 2017. lmerTest Package: Tests in Linear Mixed Effects 
1211 
Models. J Stat Softw 82:1–26. doi:10.18637/jss.v082.i13 
1212 
Larson E, McCloy D, Maddox R, Pospisil D. 2014. expyfun: Python experimental paradigm functions, 
1213 
version 2.0.0. Zenodo. doi:10.5281/zenodo.11640 
1214 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
41 
L’Engle M. 2012. A Wrinkle in Time: 50th Anniversary Commemorative Edition. Macmillan. 
1215 
Liberman MC, Epstein MJ, Cleveland SS, Wang H, Maison SF. 2016. Toward a Differential Diagnosis of 
1216 
Hidden Hearing Loss in Humans. PLOS ONE 11:e0162726. doi:10.1371/journal.pone.0162726 
1217 
Maddox RK, Lee AKC. 2018. Auditory Brainstem Responses to Continuous Natural Speech in Human 
1218 
Listeners. eNeuro 5. doi:10.1523/ENEURO.0441-17.2018 
1219 
Mesgarani N, David SV, Fritz JB, Shamma SA. 2009. Influence of Context and Behavior on Stimulus 
1220 
Reconstruction From Neural Activity in Primary Auditory Cortex. J Neurophysiol 102:3329–3339. 
1221 
doi:10.1152/jn.91128.2008 
1222 
Miller L, IV BM, Bishop C. 2017. Frequency-multiplexed speech-sound stimuli for hierarchical neural 
1223 
characterization of speech processing. US20170196519A1. 
1224 
Møller AR, Jannetta PJ. 1983. Interpretation of brainstem auditory evoked potentials: results from 
1225 
intracranial recordings in humans. Scand Audiol 12:125–133. 
1226 
Moore JK. 1987. The human auditory brain stem as a generator of auditory evoked potentials. Hear Res 
1227 
29:33–43. doi:10.1016/0378-5955(87)90203-6 
1228 
Neely ST, Norton SJ, Gorga MP, Jesteadt W. 1988. Latency of auditory brain‐stem responses and 
1229 
otoacoustic emissions using tone‐burst stimuli. J Acoust Soc Am 83:652–656. 
1230 
doi:10.1121/1.396542 
1231 
O’Sullivan AE, Lim CY, Lalor EC. 2019. Look at me when I’m talking to you: Selective attention at a 
1232 
multisensory cocktail party can be decoded using stimulus reconstruction and alpha power 
1233 
modulations. Eur J Neurosci 50:3282–3295. doi:10.1111/ejn.14425 
1234 
Pernet CR, Appelhoff S, Gorgolewski KJ, Flandin G, Phillips C, Delorme A, Oostenveld R. 2019. EEG-
1235 
BIDS, an extension to the brain imaging data structure for electroencephalography. Sci Data 6:103. 
1236 
doi:10.1038/s41597-019-0104-8 
1237 
Picton TW, Hillyard SA, Krausz HI, Galambos R. 1974. Human auditory evoked potentials. I. Evaluation of 
1238 
components. Electroencephalogr Clin Neurophysiol 36:179–190. doi:10.1016/0013-4694(74)90155-
1239 
2 
1240 
Polonenko MJ, Maddox RK. 2019. The Parallel Auditory Brainstem Response. Trends Hear 
1241 
23:2331216519871395. doi:10.1177/2331216519871395 
1242 
Prendergast G, Guest H, Munro KJ, Kluk K, Léger A, Hall DA, Heinz MG, Plack CJ. 2017. Effects of noise 
1243 
exposure on young adults with normal audiograms I: Electrophysiology. Hear Res 344:68–81. 
1244 
doi:10.1016/j.heares.2016.10.028 
1245 
R Core Team. 2020. R: A language and environment for statistical computing. Vienna, Austria: R 
1246 
Foundation for Statistical Computing. 
1247 
Rasetshwane DM, Argenyi M, Neely ST, Kopun JG, Gorga MP. 2013. Latency of tone-burst-evoked 
1248 
auditory brain stem responses and otoacoustic emissions: Level, frequency, and rise-time effects. J 
1249 
Acoust Soc Am 133:2803–2817. doi:10.1121/1.4798666 
1250 
Rudnicki M, Schoppe O, Isik M, Völk F, Hemmert W. 2015. Modeling auditory coding: from sound to 
1251 
spikes. Cell Tissue Res 361:159–175. doi:10.1007/s00441-015-2202-z 
1252 
Saiz-Alía M, Forte AE, Reichenbach T. 2019. Individual differences in the attentional modulation of the 
1253 
human auditory brainstem response to speech inform on speech-in-noise deficits. Sci Rep 9:1–10. 
1254 
doi:10.1038/s41598-019-50773-1 
1255 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
42 
Saiz-Alia M, Reichenbach T. 2020. Computational modeling of the auditory brainstem response to 
1256 
continuous speech. J Neural Eng. doi:10.1088/1741-2552/ab970d 
1257 
Scott M. 2007. The Alchemyst: the secrets of the immortal Nicholas Flamel, Book 1. New York: Listening 
1258 
Library. 
1259 
Shore SE, Nuttall AL. 1985. High‐synchrony cochlear compound action potentials evoked by rising 
1260 
frequency‐swept tone bursts. J Acoust Soc Am 78:1286–1295. doi:10.1121/1.392898 
1261 
Stapells DR, Oates P. 1997. Estimation of the Pure-Tone Audiogram by the Auditory Brainstem Response: 
1262 
A Review. Audiol Neurotol 2:257–280. doi:10.1159/000259252 
1263 
Starr A, Hamilton AE. 1976. Correlation between confirmed sites of neurological lesions and abnormalities 
1264 
of far-field auditory brainstem responses. Electroencephalogr Clin Neurophysiol 41:595–608. 
1265 
doi:10.1016/0013-4694(76)90005-5 
1266 
Strelcyk O, Christoforidis D, Dau T. 2009. Relation between derived-band auditory brainstem response 
1267 
latencies and behavioral frequency selectivity. J Acoust Soc Am 126:1878–1888. 
1268 
doi:10.1121/1.3203310 
1269 
Teoh ES, Lalor EC. 2019. EEG decoding of the target speaker in a cocktail party scenario: considerations 
1270 
regarding dynamic switching of talker location. J Neural Eng 16:036017. doi:10.1088/1741-
1271 
2552/ab0cf1 
1272 
Verhulst S, Altoè A, Vasilkov V. 2018. Computational modeling of the human auditory periphery: Auditory-
1273 
nerve responses, evoked potentials and hearing loss. Hear Res, Computational models of the 
1274 
auditory system 360:55–75. doi:10.1016/j.heares.2017.12.018 
1275 
Winer JA. 2005. Decoding the auditory corticofugal systems. Hear Res 207:1–9. 
1276 
doi:10.1016/j.heares.2005.06.007 
1277 
Zilany MSA, Bruce IC, Carney LH. 2014. Updated parameters and expanded simulation options for a 
1278 
model of the auditory periphery. J Acoust Soc Am 135:283–286. doi:10.1121/1.4837815 
1279 
 
1280 
FIGURE CAPTIONS 
1281 
Figure 1. Single subject and group average (bottom right) weighted-average auditory brainstem responses 
1282 
(ABR) to ~43 minutes of broadband peaky speech. Area for the group average shows ± 1 SEM. 
1283 
Responses were high-pass filtered at 150 Hz using a first order Butterworth filter. Waves I, III, and V of the 
1284 
canonical ABR are evident in most of the single subject responses (N = 22, 16, 22 respectively), and are 
1285 
marked by the average peak latencies on the average response. 
1286 
Figure 2. Comparison of auditory brainstem (ABR) and middle latency responses (MLR) to ~43 minutes 
1287 
each of unaltered speech and broadband peaky speech. (A) The average waveform to broadband peaky 
1288 
speech (blue) shows additional, and sharper, waves of the canonical ABR and MLR than the broader average 
1289 
waveform to unaltered speech (black). Responses were high-pass filtered at 30 Hz with a first order 
1290 
Butterworth filter. Areas show ± 1 SEM. (B) Comparison of peak latencies for ABR wave V (circles) and MLR 
1291 
waves Na (downward triangles) and Pa (upward triangles) that were common between responses to 
1292 
broadband peaky and unaltered speech. Blue symbols depict individual subjects and black symbols depict 
1293 
the mean. 
1294 
Figure 3. Comparison of grand average (N = 22) measured and modeled responses to ~43 minutes of 
1295 
broadband peaky speech. Amplitudes of the linear (dashed line) and auditory nerve (AN; dotted line) 
1296 
modelled responses were in arbitrary units, and thus scaled to match the amplitude of the measured 
1297 
response (solid line) over the 0–20 ms lags. The pre-stimulus component was present in all three responses 
1298 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
43 
using a first order 30 Hz high-pass Butterworth filter (top row), but was minimized by aggressive high-pass 
1299 
filtering with a second order 200 Hz high-pass Butterworth filter (bottom row). 
1300 
Figure 4. Comparison of responses derived by using the same type of regressor in the deconvolution. 
1301 
Average waveforms (areas show ± 1 SEM) are shown for ~43 minutes each of unaltered speech (black) and 
1302 
broadband peaky speech (blue). EEG was regressed with the (A) half-wave rectified audio and (B) pulse 
1303 
train. Responses were high-pass filtered at 30 Hz using a first order Butterworth filter. Figure 5. Comparison 
1304 
of responses to 32 minutes each of male (dark blue) and female (light blue) narrated re-synthesized 
1305 
broadband peaky speech. (A) Average waveforms across subjects (areas show ± 1 SEM) are shown for 
1306 
auditory brainstem response (ABR) time lags with high-pass filtering at 150 Hz (top), and both ABR and 
1307 
middle latency response (MLR) time lags with a lower high-pass filtering cutoff of 30 Hz (bottom). (B) 
1308 
Histograms of the correlation coefficients between responses evoked by male- and female-narrated 
1309 
broadband peaky speech during ABR (top) and ABR/MLR (bottom) time lags. Solid lines denote the median 
1310 
and dotted lines the inter-quartile range. (C) Comparison of ABR (top) and MLR (bottom) wave peak latencies 
1311 
for individual subjects (gray) and the group mean (black). ABR and MLR responses were similar to both 
1312 
types of input but are smaller for female-narrated speech, which has a higher glottal pulse rate. Peak 
1313 
latencies for female-evoked speech were delayed during ABR time lags but faster for early MLR time lags.  
1314 
Figure 6. Comparison of responses to ~43 minutes of male-narrated multiband peaky speech. (A) Average 
1315 
waveforms across subjects (areas show ± 1 SEM) are shown for each band (colored solid lines) and for the 
1316 
common component (dot-dash gray line, same waveform replicated as a reference for each band), which 
1317 
was calculated using 6 false pulse trains. (B) The common component was subtracted from each band’s 
1318 
response to give the frequency-specific waveforms (areas show ± 1 SEM), which are shown with high-pass 
1319 
filtering at 30 Hz (solid lines) and 150 Hz (dashed lines). (C) Mean ± SEM peak latencies for each wave 
1320 
decreased with increasing band frequency. Numbers of subjects with an identifiable wave are given for 
1321 
each wave and band. Details of the mixed effects models for (C) are provided in Supplementary File 1A. 
1322 
Figure 7. Comparison of responses to ~43 minutes of male-narrated peaky speech in the same subjects. 
1323 
Average waveforms across subjects (areas show ± 1 SEM) are shown for broadband peaky speech (blue) 
1324 
and for the summed frequency-specific responses to multiband peaky speech with the common component 
1325 
added (red), high-pass filtered at 150 Hz (left) and 30 Hz (right). Regressors in the deconvolution were 
1326 
pulse trains.  
1327 
Figure 8. Comparison of responses to 32 minutes each of male- and female-narrated re-synthesized 
1328 
multiband peaky speech. (A) Average frequency-specific waveforms across subjects (areas show ± 1 SEM; 
1329 
common component removed) are shown for each band in response to male- (dark red lines) and female-
1330 
narrated (light red lines) speech. Responses were high-pass filtered at 30 Hz (left) and 150 Hz (right) to 
1331 
highlight the MLR and ABR respectively. (B) Correlation coefficients between responses evoked by male- 
1332 
and female-narrated multiband peaky speech during ABR/MLR (left) and ABR (right) time lags for each 
1333 
frequency band. Black lines denote the median. (C) Mean ± SEM peak latencies for male- (dark) and 
1334 
female- (light) narrated speech for each wave decreased with increasing frequency band. Numbers of 
1335 
subjects with an identifiable wave are given for each wave, band and narrator. Lines are given a slight 
1336 
horizontal offset to make the error bars easier to see. Details of the mixed effects models for (C) are 
1337 
provided in Supplementary File 1B. 
1338 
Figure 9. Comparison of responses to ~60 minutes each of male- and female-narrated dichotic multiband 
1339 
peaky speech with standard audiological frequency bands. (A) Average frequency-specific waveforms 
1340 
across subjects (areas show ± 1 SEM; common component removed) are shown for each band for the left 
1341 
ear (dotted lines) and right ear (solid lines). Responses were high-pass filtered at 30 Hz. (B) Left-right ear 
1342 
correlation coefficients (top, averaged across gender) and male-female correlation coefficients (bottom, 
1343 
averaged across ear) during ABR time lags (0–15 ms) for each frequency band. Black lines denote the 
1344 
median. (C) Mean ± SEM wave V latencies for male- (dark red) and female-narrated (light red) speech for 
1345 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
44 
the left (dotted line, cross symbol) and right ear (solid line, circle symbol) decreased with increasing 
1346 
frequency band. Lines are given a slight horizontal offset to make the error bars easier to see. Details of 
1347 
the mixed effects model for (C) are provided in Supplementary File 1C. 
1348 
Figure 10. Cumulative proportion of subjects who have responses with ≥ 0 dB SNR as a function of 
1349 
recording time. Time required for unaltered (black) and broadband peaky speech (dark blue) of a male 
1350 
narrator are shown for 22 subjects in the left plot, and for male (dark blue) and female (light blue) 
1351 
broadband peaky speech is shown for 11 subjects in the right plot. Solid lines denote SNRs calculated 
1352 
using variance of the signal high-pass filtered at 30 Hz over the ABR/MLR interval 0–30 ms, and dashed 
1353 
lines denote SNR variances calculated on signals high-pass filtered at 150 Hz over the ABR interval 0–15 
1354 
ms. Noise variance was calculated in the pre-stimulus interval −480 to −20 ms. 
1355 
Figure 11. Cumulative proportion of subjects who have frequency-specific responses (common component 
1356 
subtracted) with ≥ 0 dB SNR as a function of recording time. Acquisition time was faster for male (left) than 
1357 
female (right) narrated multiband peaky speech with (A) 4 frequency bands presented diotically, and with 
1358 
(B) 5 frequency bands presented dichotically (total of 10 responses, 5 bands in each ear). SNR was 
1359 
calculated by comparing variance of signals high-pass filtered at 150 Hz across the ABR interval of 0–15 
1360 
ms to variance of noise in the pre-stimulus interval −480 to −20 ms. 
1361 
Figure 12. The range of lags can be extended to allow early, middle and late latency responses to be 
1362 
analyzed from the same recording to broadband peaky speech. Average waveforms across subjects (areas 
1363 
show ± 1 SEM) are shown for responses measured to 32 minutes of broadband peaky speech narrated by 
1364 
a male (dark blue) and female (light blue). Responses were high-pass filtered at 1 Hz using a first order 
1365 
Butterworth filter, but different filter parameters can be used to focus on each stage of processing. 
1366 
Canonical waves of the ABR, MLR and LLR are labeled for the male-narrated speech. Due to adaptation, 
1367 
amplitudes of the late potentials are smaller than typically seen with other stimuli that are shorter in 
1368 
duration with longer inter-stimulus intervals than our continuous speech. Waves I and III become more 
1369 
clearly visible by applying a 150 Hz high-pass cutoff. 
1370 
Figure 13. Unaltered speech waveform (top left) and spectrogram (top right) compared to re-synthesized 
1371 
broadband peaky speech (middle left and right) and multiband peaky speech (bottom left and right). 
1372 
Comparing waveforms shows that the peaky speech is as “click-like” as possible, while comparing the 
1373 
spectrograms shows that the overall spectrotemporal content that defines speech is basically unchanged 
1374 
by the re-synthesis. A naïve listener is unlikely to notice that any modification has been performed, and 
1375 
subjective listening confirms the similarity. Yellow/lighter colors represent larger amplitudes than 
1376 
purple/darker colors in the spectrogram. See supplementary files for audio examples of each stimulus type 
1377 
for both narrators. 
1378 
Figure 14. Relative mean-squared magnitude in decibels of multiband peaky speech with 4 filter bands 
1379 
(left) and 5 filter bands (right) for male-(blue) and female-(orange) narrated speech. The full audio 
1380 
comprises unvoiced and re-synthesized voiced sections, which was presented to the subjects during the 
1381 
experiments. The other bands reflect the relative magnitude of the voiced sections (voiced only), and each 
1382 
filtered frequency band. 
1383 
Figure 15. Spectral coherence of pulse trains for multiband peaky speech narrated by a male (left) and 
1384 
female (right). Spectral coherence was computed across 1 s slices from 60 unique 64 s multiband peaky 
1385 
speech segments (3,840 total slices) for each combination of bands. Each light gray line represents the 
1386 
coherence for one band comparison. (A) There were 45 comparisons across the 10-band (audiological) 
1387 
speech used in experiment 3 (5 frequency bands x 2 ears). The lowest band was unshifted and the other 9 
1388 
bands had static frequency shifts. (B) There were 6 comparisons across 4 pulse trains of the bands in the 
1389 
pilot experiment, which all had dynamic random frequency shifts.  Pulse trains (i.e., the input stimuli, or 
1390 
regressors, for the deconvolution) were frequency-dependent (coherent) below 72 Hz for the male 
1391 
multiband speech and 126 Hz for the female multiband speech. 
1392 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
45 
Figure 16. Octave band filters used to create re-synthesized broadband peaky speech (left, blue), diotic 
1393 
multiband peaky speech with 4 bands (middle, red), and dichotic multiband peaky speech using 5 bands 
1394 
with audiological center frequencies (right, red). The last band (2nd, 5th, 6th respectively, black line) was 
1395 
used to filter the high-frequencies of unaltered speech during mixing to improve the quality of voiced 
1396 
consonants. The designed frequency response using trapezoids (top) were converted into the time-domain 
1397 
using IFFT, shifted and Nuttall windowed to create impulse responses (middle), which were then used to 
1398 
assess the actual frequency response by converting into the frequency domain using FFT (bottom). 
1399 
Figure 17. Left: A segment of broadband peaky speech stimulus (top) and the corresponding glottal pulse 
1400 
train (bottom) used in calculating the broadband peaky speech response. Right: An example broadband 
1401 
peaky speech response from a single subject. The response shows ABR waves I, III, and V at ~3, 5, 7 ms 
1402 
respectively. It also shows later peaks corresponding to thalamic and cortical activity at ~17 and 27 ms 
1403 
respectively. 
1404 
 
1405 
FIGURE SUPPLEMENTS 
1406 
Figure 4-figure supplement 1. Comparison of responses derived by using the half-wave rectified audio as 
1407 
the regressor in the deconvolution with electroencephalography (EEG) recorded in response to ~43 
1408 
minutes of speech. Average waveforms (areas show ± 1 SEM) are shown for EEG recorded to unaltered 
1409 
speech (black) relative to EEG recorded to multiband peaky speech (red). Responses were high-pass 
1410 
filtered at 30 Hz using a first order Butterworth filter.  
1411 
Figure 6-figure supplement 1. Comparison of responses to 64 minutes each of male- (left) and female-
1412 
narrated (right) multiband peaky speech created with the dynamic random frequency shift method. (A) 
1413 
Weighted-average waveforms for one subject are shown for each band (colored solid lines) and for the 
1414 
common component (dot-dashed gray line, same waveform replicated as a reference for each band), 
1415 
which was calculated using 6 false pulse trains. (B) The common component was subtracted from each 
1416 
band’s response to give the frequency-specific waveforms, which are shown with high-pass filtering at 30 
1417 
Hz (solid lines) and 150 Hz (dashed lines). Responses from all four bands show more consistent 
1418 
resemblance to the common component, indicating that this method is effective at reducing stimulus-
1419 
related bias. However, differences still remain in the lowest frequency band for latencies >30 ms, 
1420 
suggesting that this new method reveals true underlying low-frequency neural activity that is unique. 
1421 
Figure 15-figure supplement 1. Comparison of the common component derived from the average 
1422 
response to 6 fake pulse trains that were created using static frequency shifts (solid, darker lines; used in 
1423 
the paper) or dynamic random frequency shifts (dashed, lighter lines, pilot data and suggested in methods). 
1424 
Responses were to 32 minutes each of male- (left) and female- (right) narrated re-synthesized diotic 
1425 
multiband peaky speech. Areas show ±1 SEM. The EEG to diotic multiband peaky speech (4-bands) was 
1426 
regressed with 6 fake pulse trains created using the static shifts (used in the paper; the same common 
1427 
component displayed in Figure 4), as well as 6 fake pulse trains created using the dynamic random 
1428 
frequency shift method (used to create the common component in Supplementary Figure 2). 
1429 
Figure 15-figure supplement 2. Multiband stimuli responses for male (left) and female (right) derived by 
1430 
deconvolving the absolute value of the dichotic (stereo) multiband peaky audio (from experiment 3) with the 
1431 
10 associated pulse trains– 5 pulse trains were used for each band in each ear (“correct” pulse trains, top 
1432 
row). Each pulse train acted as a “wrong” pulse train for the associated band in the other ear (bottom row). 
1433 
Left ear responses are shown by dotted lines and right ear responses by solid lines. Higher frequency 
1434 
bands are shown by lighter red colors. The non-zero responses only occurred when the correct pulse train 
1435 
was paired with the correct audio. Both male- and female-narrated speech responses symmetrically 
1436 
surrounded zero ms and were largest for the first (lowest frequency) band when the correct, but not fake, 
1437 
pulse trains were used as the regressor in the deconvolution. 
1438 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
 
46 
 
1439 
AUDIO FILES 
1440 
Audio 1. Unaltered speech sample from the male narrator (The Alchemyst; Scott, 2007). 
1441 
Audio 2. Broadband peaky speech sample from the male narrator (The Alchemyst; Scott, 2007). 
1442 
Audio 3. Multiband peaky speech sample from the male narrator (The Alchemyst; Scott, 2007). 
1443 
Audio 4. Unaltered speech sample from the female narrator (A Wrinkle in Time; L’Engle, 2012). 
1444 
Audio 5. Broadband peaky speech sample from the female narrator (A Wrinkle in Time; L’Engle, 2012). 
1445 
Audio 6. Multiband peaky speech sample from the female narrator (A Wrinkle in Time; L’Engle, 2012). 
1446 
 
1447 
SUPPLEMENTARY FILES 
1448 
Supplementary File 1A. LMER model formula and summary output for multiband peaky speech in 
1449 
experiment 1. 
1450 
Supplementary File 1B. LMER model formula and summary output for multiband peaky speech in 
1451 
experiment 2. 
1452 
Supplementary File 1C. LMER model formula and summary output for multiband peaky speech in 
1453 
experiment 3. 
1454 
.
CC-BY-ND 4.0 International license
available under a
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made 
The copyright holder for this preprint
this version posted January 12, 2021. 
; 
https://doi.org/10.1101/2020.08.20.258301
doi: 
bioRxiv preprint 
